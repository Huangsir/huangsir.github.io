<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[huangsir]]></title><description><![CDATA[huangsir's blog]]></description><link>http://ruibo.me</link><generator>RSS for Node</generator><lastBuildDate>Fri, 27 Oct 2017 09:52:36 GMT</lastBuildDate><atom:link href="http:/ruibo.me/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Fri, 27 Oct 2017 09:52:34 GMT</pubDate><item><title><![CDATA[程序员哥哥第一次与龙虾妹妹]]></title><description><![CDATA[<p>&lt;!-- datetime: 2017-07-22 01:12:11 --&gt;
</p>
<p>JD忽然由打折大龙虾，买一只回来炖了，龙虾妹妹，哥哥就不客气了~</p>
<p>&lt;!-- more --&gt;
</p>
<p>前几天，JD爸爸忽然大发神威，500g的波士顿龙虾1只79，2只包邮。老夫毫不犹豫的就剁了。<img src="https://user-images.githubusercontent.com/3870517/28492382-eadf8cbc-6f34-11e7-84b9-b118f5339e8d.png" alt="image"></p>
<p>结果这几天ApplePay也发大招，必胜客满160减80，果断约小伙伴们连吃了几晚必胜客<img src="https://user-images.githubusercontent.com/3870517/28492388-20d3ad12-6f35-11e7-866d-57df39be9dbb.png" alt="image">。</p>
<p>结果到货的龙虾就丢在公司冰箱里忘记拿了<img src="https://user-images.githubusercontent.com/3870517/28492391-3c24dde8-6f35-11e7-9e4b-5b900142f9f5.png" alt="image">。
结果放了一晚上后，今天哥哥拿回家打开一看，居然是活的...<img src="https://user-images.githubusercontent.com/3870517/28492404-b145c498-6f35-11e7-8acd-0c1d7b913c4c.png" alt="image">，牛逼了word哥，第一次在JD上买到活的东西而且物流居然还能这样送过来。<img src="https://user-images.githubusercontent.com/3870517/28492412-d7d78f60-6f35-11e7-984e-bc2e4a934bc9.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063781-b4c0de48-ba3d-11e7-9eee-ed00d3dd52c3.PNG" alt="img_6005"></p>
<p>激动的我赶紧查了一下怎么蒸龙虾<img src="https://user-images.githubusercontent.com/3870517/28492418-094170e8-6f36-11e7-90dd-91ddb35f38bc.png" alt="image">，贴心的卖家已经发了处理龙虾的示意图。</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063790-bb32dcae-ba3d-11e7-966e-397afe72a67b.JPG" alt="img_6008"></p>
<p>嗯？！有点奇怪，怎么还要爆菊。<img src="https://user-images.githubusercontent.com/3870517/28492577-859e83ae-6f38-11e7-8be5-c74cce8272ed.png" alt="image"></p>
<p>好吧，既然如此，那龙虾妹妹今晚你就从了哥哥吧。<img src="https://user-images.githubusercontent.com/3870517/28492517-d62c771e-6f37-11e7-9e14-ed8fffc9b157.png" alt="image"></p>
<p>哥哥让龙虾妹妹轻轻的躺在砧板上不要动~。哥哥也是第一次，没有什么经验，但是哥哥不会弄疼你的。<img src="https://user-images.githubusercontent.com/3870517/28492540-4425cf4a-6f38-11e7-836b-77fa74a03931.png" alt="image"></p>
<p>龙虾妹妹死活都不从呀，翻来翻去的，哥哥也好怕...</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063803-c216d4c6-ba3d-11e7-89fa-1662b263024b.PNG" alt="img_6007"></p>
<p>哥哥也是注意卫生的人，哥哥决定先带套。<img src="https://user-images.githubusercontent.com/3870517/28492540-4425cf4a-6f38-11e7-836b-77fa74a03931.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063807-c8688ee6-ba3d-11e7-9e60-da8247c9fa2a.JPG" alt="img_5979"></p>
<p>带套之后，感觉就没有那么强烈了。<img src="https://user-images.githubusercontent.com/3870517/28492591-ea8c22f8-6f38-11e7-84fc-3ab528ff0b2f.png" alt="image"></p>
<p>哥哥掏出了凶器，深深的x入到龙虾妹妹的身体里。<img src="https://user-images.githubusercontent.com/3870517/28492586-ba400d58-6f38-11e7-936f-d78405e6c432.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063815-cec041a8-ba3d-11e7-878d-82321a714b71.JPG" alt="img_5982"></p>
<p>我已经无法直视这跟筷子了...然而老夫手头只有一双筷子，等下还要用它来夹菜...<img src="https://user-images.githubusercontent.com/3870517/28492622-6c7f69b4-6f39-11e7-927c-10fa24cc797a.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063909-0b55b9e0-ba3e-11e7-990b-e244d0cab632.JPG" alt="img_5983"></p>
<p>请叫我哈利波sir<img src="https://user-images.githubusercontent.com/3870517/28492630-87e03df0-6f39-11e7-925e-ac47dc0756e0.png" alt="image"></p>
<p>龙虾的尿真的很臭，老夫晚上吃的必胜客已经跑到喉咙了...<img src="https://user-images.githubusercontent.com/3870517/28492577-859e83ae-6f38-11e7-8be5-c74cce8272ed.png" alt="image">而且放出来的尿会变成果冻的状态粘在台面上很难处理很恶心，建议同学们直接放到进水池里冲走。</p>
<p>经过一轮战斗之后，龙虾妹妹精疲力尽的躺在<strong><del>床</del></strong>砧板上睡着了，老夫要打盘农药压压惊~<img src="https://user-images.githubusercontent.com/3870517/28492622-6c7f69b4-6f39-11e7-927c-10fa24cc797a.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063823-d5e446dc-ba3d-11e7-848a-65e97bc7e31e.JPG" alt="img_5984"></p>
<p>请脑补忽视略脏的台面，那些黄褐色的液体就是放出来的虾尿...</p>
<p>15分钟迅速解决了对面一帮扑街佬之后，哥哥已经重振雄风，向着已经沉睡的龙虾妹妹伸出了<strong><del>淫爪</del></strong>双手。</p>
<p>黑色的那个有人说是虾子(一粒一粒的)，有人说是精囊，但是老夫还是扔掉了...闻起来有种恶心的臭味<img src="https://user-images.githubusercontent.com/3870517/28492740-3e1d2c4e-6f3b-11e7-9075-a615c5478c30.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063832-dd02f8a0-ba3d-11e7-8790-2d1ac9943e3a.JPG" alt="img_5987"></p>
<p>用手小心翼翼的摘掉胃囊以后，同样小心翼翼地把脑子摘下来。</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063930-19211146-ba3e-11e7-95bb-1cf04faae50a.JPG" alt="img_5988"></p>
<p>龙虾脑蒸蛋…吃了可不可以变成最强大脑~<img src="https://user-images.githubusercontent.com/3870517/28492735-1e661d0c-6f3b-11e7-981e-3c826cbae5e3.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063841-e0fe7150-ba3d-11e7-8dc6-a1a2cadffeda.JPG" alt="img_5989"></p>
<p>最后就是按照教科书上的开边，老夫的刀比较钝，开得比较挫，大家可以自行在脑部补充优美画面<img src="https://user-images.githubusercontent.com/3870517/28492768-d8aeea04-6f3b-11e7-85c5-8c2cb30e4e76.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063865-ed7dab4e-ba3d-11e7-87e1-19a0b5c2397f.JPG" alt="img_5990"></p>
<p>砍碎</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063854-e6a4bb3c-ba3d-11e7-82a8-a26e6dcea8a6.JPG" alt="img_5991"></p>
<p>炒一点蒜蓉加酱油加蚝油撒上去</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063881-f36f1196-ba3d-11e7-9a02-cf3c83f8d984.JPG" alt="img_5992"></p>
<p>随便蒸个10分钟</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32063891-f80b5f02-ba3d-11e7-931f-94f927bf6210.jpg" alt="fullsizerender 4"></p>
<p>程序员哥哥的第一次</p>
<p>...</p>
<p>...</p>
<p>...</p>
<p>蒸龙虾，还挺成功的~</p>
]]></description><link>http:/ruibo.me/posts/268821650.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268821650.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Thu, 26 Oct 2017 17:57:45 GMT</pubDate></item><item><title><![CDATA[我的2016]]></title><description><![CDATA[<p>&lt;!-- datetime: 2017-02-05 03:12:33 --&gt;
</p>
<p>2016年过去了，一年一年的，总该记录点什么</p>
<p>&lt;!-- more --&gt;
</p>
<p>首先必须吐槽一下春节实在是太早了，很多事情都要在节前赶完，第一次这么不想放假...</p>
<p>16年发生了很多改变生活的大事件，滴滴和Uber合并，Mobike，阿法狗，人工智能等等，现在人工智能越炒越热，就感觉未来就是人工智能的时代一样。</p>
<p>16年前4个季度基本都在做DSP相关的东西，不断的接入各种ADX，优化性能，做DSP后台，整理数据，抽象化组件，迁移服务器等等。到了Q4之后，接手了整个老的国内广告业务，但是由于老的SDK业务比较稳定，新的DSP业务又一直难有起色，所以Q4的大部分时间其实主要还是在跟进DSP的工作。数据挖掘，CTR，等等相关的工作也一并涵盖在我的DSP小分队里。</p>
<p>粗略会想，似乎16年也就干了这么一件事情。</p>
<p>分心</p>
<p>我确实有些贪多了，想做成DSP，又想阅读Paper，专研数据挖掘、CTR预估、动态估价的模型算法。在要带团队，跟需求，做绩效的同时，还想插手基础组件的研发，最后实在是有些分心乏力。睡眠质量一度下降。</p>
<p>工作中心调整后，我的时间也被打得比较零散，很多时候也不敢去碰代码，已经大量的事件表明，只要我插手一段代码，那它的进度已定是最慢的（摔）</p>
<p>到最后甚至还出现一些记忆错乱的情况，总是感觉同一个话题，好似和同事们已经讨论过好几次了，但是好像总是在两个答案之间反复选择。</p>
<p>但是即便如此，我也还是插手了那么多的东西，通过不断卖萌，全公司大部分的后端开发都纳入到我的组织下。通过子又生子，子又生孙的方法，终于还是将爪牙伸到了这些方向上。除了阅读Paper这个必须亲力亲为又费时费神的工作。</p>
<p>插手这么多事情也使我意识到我平时的作息规律对于时间的利用率还是极低的，如果能够进一步提高对时间的利用率，把Paper读下来应该也不是什么难事，这将是我今年的目标之一。</p>
<p>技术与业务</p>
<p>我始终认为，技术人员必须要非常了解自己所处行业的业务形态、发展和未来演变。这样可以使得你能够保持高度的敏感性，所以技术人员也必须是要非常与业务挂钩的，如果不懂得业务，那便无从下手，毕竟隔行如隔山。</p>
<p>为什么我会要插手那么多的模块？做数据挖掘的，不知道业务形态特点，只能干巴巴的套公式，而这东西为什么好，又为什么不好，不能得到合理的解释的话，最终也只是一个黑匣子。</p>
<p>做基础组件的，我始终认为模块和组件都是从实际的业务逻辑中不断抽象共通的部分逐渐剥离而成的，既然是这样，不做业务的人，哪里有资格能做基础组件？</p>
<p>所谓核心技术</p>
<p>做了这么多年技术到16年，我深深的感受到，技术的红利随着互联网爆炸，尤其是各大高校鸡血一样的不断向社会输送着各样的人才之后，红利已经开始渐渐变薄了。而且隐隐的还有一种感觉，实现技术成就感，本身就是一种削弱技术红利的事情。</p>
<p>16年反复被人问了很多次，你们公司的核心技术优势在哪里？这个问题我反复想了很久，当一个人想我提及核心技术这样一个名词时，我脑海中浮现的第一个词是人无我有。但回过头来再想想，我们这个行业，有哪家真的在纯技术的领域做到了人无我有？好像想不出来。强如BAT，真的是人无我有么？就淘宝而言，纯技术级别的复制，真有以我司的技术实力无法实现的点么？好似没有。搜索引擎很难么？好像也没有很难。微信这样的APP只有腾讯做得出么？好像也不是。</p>
<p>这些技术都很难么？好像都不难，但是为什么不是我们？为什么头条今年就可以做60亿？为什么我就喜欢用网易云音乐？它们有什么技术是其他人都做不到的么？并没有。那它们的核心技术优势在哪里？我们的核心技术优势又在哪里？</p>
<p>想了想，核心技术只有2条。一是开发效率高，二是数据积累多。而数据积累并不是一项纯技术的工作，他还涉及到了很多黑科技，灰色产业，业务发展的好坏，业务形态等。所以纯纯纯萃的技术壁垒，其实只有开发效率。</p>
<p>同质化的产品，你比别人早2个月完工，你就可以早一步打入市场，有钱的话甚至能把受众先洗一遍。在这个红利优势下，只要不犯什么大错被人赶超，后进入者却是很难竞争。而为了让对手慢一些完工，直接高价把主程挖走是最省钱的方法，各种直播间互相打仗的季节里这样下三滥的手段各公司不知用了多少。</p>
<p>所以如何能够打造我们的核心竞争力？一是自身实力够硬，能够快速编码不带四处翻Google不带出Bug能一次撸完直接打到上线Level的。二是对行情有一定的敏锐性，当人家说了一个想法时，能够迅速将一整个市场到产品到技术开发成性上线推出市场然后二次迭代都在脑子里推导出来。这样才不会一脸懵逼的开发东西。</p>
<p>当然我说这话显得有些马后炮......</p>
<p>升级太快</p>
<p>16年对我来说非常的不一般，在16年以前我顶多也就是带两三个小弟一起干活。然而现在，我不但有一堆小弟，小弟还有小弟，到Q4重新接手老业务的时候已经几十个人了。加上年初作死的插手了很多项目，导致一堆东西需要管理。连本来Review代码的时间也被迫只能在晚上来做。</p>
<p>我一直都是一个脾气很暴躁的人，这一块一直是我的一个短板，而现在带队之后，又要不断的显得和蔼，和同学们拉家常，谈心。然后回过头来还要做绩效，发工资。直接从一个用键盘写代码的变成了一个用人写代码的。</p>
<p>回想前几年，其实是有过不少机会的，也有意识到有这些机会，只是当时潜意识的放弃掉了，因为也是意识到一旦这样做了，那就无法再做一个安安静静写代码的美男子。然后自爷走后，就已经没有了退路。因此也不能再停留舒适区了，需要尽快将自身短板补平。</p>
<p>大危机</p>
<p>好吧每年都有大危机，一点都不意外好么。而且每年都是那种要死要死的感觉，大家的信心都不足。之前几年两耳不闻窗外事，对于这些事情只是有感，虽然曾经也有萌生过离职的想法。今年我们事业部可谓是首当其冲，DSP业务也是挫折不断，KPI完成率连60分都达不到，鸭梨山大。再想想前几年，真是不当家的不知道菜米油盐贵。</p>
<p>投资竞价大不景气，强如小米也因为前几年估值太高，导致在今年投资寒冬下难以度过下一轮估值的问题。那我们就更没什么好说的了，大环境下如此也没什么好抱怨的。</p>
<p>但是在这个环境下，今年直播行业，经济行业，短贷行业如此火热我们居然连雨露均沾都没有，也确实有点不像话。</p>
<p>大危机年年都有，不想说了。</p>
<p>对于我自己</p>
<p>其实要吐槽的东西还是挺多的，每年都可以吐槽自己几百遍，也会在心里吐槽不给力的同事几百遍。但是吐槽归吐槽，再怎么不爽，短板还是需要补齐，时间规划还是需要优化，各端该插手还是需要插手，脾气还是需要再改改，如果能再对市场更敏锐一些，不那么后知后觉一些，就更好了。</p>
]]></description><link>http:/ruibo.me/posts/268819961.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268819961.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 01:33:04 GMT</pubDate></item><item><title><![CDATA[git flow release finish的时候自动生成Changelog]]></title><description><![CDATA[<p>&lt;!-- datetime: 2016-02-23 19:22:19 --&gt;
</p>
<p>大部分人应该都没有写Changelog的习惯吧？我也没有...</p>
<p>&lt;!-- more --&gt;
</p>
<p>git有提供hook的方法，可以实现帮助我们实现许多自定义的功能。在git flow大热之后，许多的企业项目，都采用git flow的流程来作为项目开发的标准流程。</p>
<p>有一天老夫在网上逛荡的时候，注意到一些服务每次在发布版本的时候，都有提供详细的Changelog，然而老夫自己的项目里基本上是不会有这种东西的，而且我感觉也没有人有兴趣每天在上面更新项目的Changelog。作为一个极（ma）客（nong），我们应该是用技术的手法来减少工作。</p>
<p>于是乎，我就想到了在git flow发布release的时候，自动生成一份Changelog。</p>
<p>在网上翻了翻，其实有不少网友已经有提供了现成的脚本，老夫随意拿了一份来，改了改~~</p>
<p>git flow在安装的时候，本身已经对git的hook进行了一定的扩展，feature，release，hotfix都有start和finish的hook。我就选择使用release的finish的hook来实现自动生成Changelog的功能。</p>
<p>我希望能够达到的效果是：当我发布一个版本时（git flow release finish），程序能够自动将上一个版本，到这一次发版之间，所有的commit，全部输出到Changelog.md这个文件上，然后将这个文件的修改commit到版本库里。</p>
<p>我们新建一个项目，就叫做test_hook好了，然后在项目里使用git init初始化git目录，然后使用git flow init初始化git flow配置。</p>
<p>这时进入项目里.git/hooks，可以看到里面有一些hook的sample，如果没有找到这个目录，可以自己make一个。</p>
<p>然后我们在hooks目录下添加一个文件名为pre-flow-release-finish的文件。</p>
<p>文件脚本如下：</p>
<pre><code>#!&#x2F;bin&#x2F;bash
#
# Runs before git flow release finish
#
# Positional arguments:
# $1    The version (including the version prefix)
# $2    The origin remote
# $3    The full branch name (including the release prefix)
#
# The following variables are available as they are exported by git-flow:
#
# MASTER_BRANCH - The branch defined as Master
# DEVELOP_BRANCH - The branch defined as Develop
#
VERSION&#x3D;$1
ORIGIN&#x3D;$2
BRANCH&#x3D;$3

if [ &#x60;echo &quot;$VERSION&quot; | grep -P &quot;v\d+.+&quot;&#x60; !&#x3D; $VERSION ]; then
    exit 0
fi

REPOADDR&#x3D;&#x60;git config --get remote.origin.url | sed -s &#39;s&#x2F;.git$&#x2F;&#x2F;g&#39;&#x60;
if [ ${REPOADDR:0:3} &#x3D;&#x3D; &quot;git&quot; ]; then
    REPOADDR&#x3D;&#x60;echo $REPOADDR | sed -s &#39;s&#x2F;:&#x2F;\&#x2F;&#x2F;g;s&#x2F;git\@&#x2F;https:\&#x2F;\&#x2F;&#x2F;g&#39;&#x60;
fi
LINKADDR&#x3D;$REPOADDR&#x2F;commit&#x2F;

rm Changelog.md 2&gt;&#x2F;dev&#x2F;null
while read TAG; do
    if [ ! $CURRENT ]; then
        REV&#x3D;&quot;--all&quot;
        CURRENT&#x3D;1
        echo &quot;     &quot; &gt;&gt; Changelog.md
        echo &quot;*$VERSION (CURRENT)*&quot; &gt;&gt; Changelog.md
        echo &quot;---&quot; &gt;&gt; Changelog.md
    else
        REV&#x3D;$TAG
        echo &quot;     &quot; &gt;&gt; Changelog.md
        echo *$NEXT* &gt;&gt; Changelog.md
        echo &quot;---&quot; &gt;&gt; Changelog.md
    fi

    echo &quot;     &quot; &gt;&gt; Changelog.md
    git log --no-merges --date&#x3D;short  --pretty&#x3D;format:&quot;- %ad (%an) %s -&gt; [view commit](${LINKADDR}%h)&quot; $TAG..$NEXT | grep -v &quot;Edit Changelog.md&quot; &gt;&gt; Changelog.md
    echo &quot;     &quot; &gt;&gt; Changelog.md

    NEXT&#x3D;$TAG
done &lt; &lt;(git for-each-ref --sort&#x3D;&#39;*authordate&#39; --format&#x3D;&#39;%(tag)&#39; refs&#x2F;tags | grep &#39;v\d+.+&#39; -P | tac)

COMMITID&#x3D;&#x60;git rev-list $NEXT | head -n 1&#x60;
LINKADDR&#x3D;$REPOADDR&#x2F;commit&#x2F;$COMMITID
echo &quot;     &quot; &gt;&gt; Changelog.md
echo *$NEXT* &gt;&gt; Changelog.md
echo &quot;---&quot; &gt;&gt; Changelog.md
echo &quot;     &quot; &gt;&gt; Changelog.md
git log --no-merges --date&#x3D;short  --pretty&#x3D;format:&quot;- %ad (%an) %s -&gt; [view commit](${LINKADDR}%h)&quot; $NEXT &gt;&gt; Changelog.md
echo &quot;     &quot; &gt;&gt; Changelog.md

git add Changelog.md
git commit -am &quot;Edit Changelog.md&quot;

exit 0</code></pre><p>上面的逻辑表示他会读取这次release的tag，如果tag匹配v\d+.+，如v1.1这样的格式，那么就可以继续进项后续的流程。也就是说，我们release的tag，必须是满足类似v1.1或v2.12.4这样的格式，才能够正确被脚本读取，其余格式的tag都会被直接忽略，当做一个普通的commit来处理。</p>
<p>因为我们的Changelog里，每个commit id要能够对应到具体的页面上。而.git/config有可能记录的是以git为scheme的URI，所以这里需要一个简陋的转换，把git://的地址转为使用<a href="https://的地址。">https://的地址。</a></p>
<p>然后就是把旧的Changelog.md删掉，同时生成新的Changelog.md。</p>
<p>最后就是把这个新的Changelog.md的修改commit上去，之后git flow release finish的流程就完成了。这时的版本库里就携带有最新的Changelog信息。</p>
<p>更多对git hook的了解可以参阅<a href="https://git-scm.com/book/zh/v1/%E8%87%AA%E5%AE%9A%E4%B9%89-Git-Git%E6%8C%82%E9%92%A9">官方文档</a></p>
]]></description><link>http:/ruibo.me/posts/268819635.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268819635.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:22:27 GMT</pubDate></item><item><title><![CDATA[基于issue的Github Page工具]]></title><description><![CDATA[<p>&lt;!--datetime: 2016-02-18 22:09:12--&gt;
</p>
<p>换了这个用commit issue的方法来发blog的工具，感觉还是挺不错的。把之前的page全部都迁移过来了，虽然时间有点对不上，不过这点小事也不用太care。</p>
<p>&lt;!-- more --&gt;
</p>
<p>基于Issue的blog有两种，一种是在前端页面直接调用github api加载数据然后现场渲染页面的。另一种是线下build完之后在用git push上传的。</p>
<p>原本第一种挺好的，但是他的核心劣势在于没有seo，没办法，只能在自己的小服务器上部署一个github hook来定期build了-，-#</p>
]]></description><link>http:/ruibo.me/posts/268819030.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268819030.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:24:29 GMT</pubDate></item><item><title><![CDATA[iphone用ikev2搭梯子]]></title><description><![CDATA[<p>&lt;!-- datetime: 2015-10-24 21:38:19 --&gt;
</p>
<p>一种个人感觉比较小众的科学Shangwang的方式</p>
<p>&lt;!-- more --&gt;
</p>
<p>想到要为我的iphone翻墙原因有两个。一是前几天出门旅游，在玩instagram；二是我一直在玩ingress，但是手头iphone翻墙实在不方便，虽然在家大部分时间还是用Android机开画图工具-，-#</p>
<p>不过始终，觉得为自己的iphone弄个翻墙还是很有必要的。</p>
<p>为此我尝试了使用openvpn。但发现openvpn在如果是连接国内ip的server是十分稳定的，但是如果连接国外的openvpn服务，就会经常被block掉连接。</p>
<p>这样我很不爽，出门打架，或者面基的时候被block了，还要花半天时间来连接服务器，这尼玛不得坑死我。</p>
<p>于是我就发现了ikev2这套协议。</p>
<p>选用的方法是在在vps上部署<code>strongswan</code>，部署的方法可以见<a href="https://gist.github.com/losisli/11081793">这篇文章</a></p>
<p>然而相比于大部分blog里直接wget下来安装的做法，本人喜欢使用apt-get来安装。</p>
<hr>
<p><strong>安装strongswan</strong></p>
<pre><code>apt-get install build-essential     #编译环境
aptitude install libgmp10 libgmp3-dev libssl-dev pkg-config libpcsclite-dev libpam0g-dev     #编译所需要的软件
apt-get install strongswan</code></pre><hr>
<p><strong>生成证书</strong></p>
<p>生成根证书</p>
<pre><code>ipsec pki --gen --outform pem &gt; caKey.pem
ipsec pki --self --in caKey.pem --dn &quot;C&#x3D;CN, O&#x3D;myNetwork, CN&#x3D;myCA&quot; --ca --outform pem &gt; caCert.pem</code></pre><p>生成服务器证书</p>
<pre><code>ipsec pki --gen --outform pem &gt; serverKey.pem
ipsec pki --pub --in serverKey.pem | ipsec pki --issue --cacert caCert.pem --cakey caKey.pem --dn &quot;C&#x3D;CN, O&#x3D;myNetwork, CN&#x3D;&lt;这里要填写服务器的ip或者域名&gt;&quot; --san&#x3D;&quot;&lt;这里要填写服务器的ip或者域名&gt;&quot; --flag serverAuth --flag ikeIntermediate --outform pem &gt; serverCert.pem</code></pre><p>生成客户端证书</p>
<pre><code>ipsec pki --gen --outform pem &gt; clientKey.pem
ipsec pki --pub --in clientKey.pem | ipsec pki --issue --cacert caCert.pem --cakey caKey.pem --dn &quot;C&#x3D;CN, O&#x3D;strongSwan, CN&#x3D;iphoneClient&quot; --outform pem &gt; clientCert.pem</code></pre><p>很多博客里会说让生成p12证书，然而在iOS8以上好像并不需要这个东西。</p>
<p>将证书Copy到对应的目录下，使用apt-get来安装strongswan的话，证书需要copy到/etc目录下</p>
<pre><code>sudo mv caCert.pem &#x2F;etc&#x2F;ipsec.d&#x2F;cacerts&#x2F;
sudo mv serverCert.pem &#x2F;etc&#x2F;ipsec.d&#x2F;certs&#x2F;
sudo mv caKey.pem &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;
sudo mv serverKey.pem &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;
sudo mv clientKey.pem &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;</code></pre><hr>
<p><strong>配置strongswan</strong></p>
<p>因为我只需要让我的iphone能够翻墙，所以我这里只有iphone的配置。</p>
<p>编辑<code>/etc/ipsec.conf</code></p>
<pre><code># ipsec.conf - strongSwan IPsec configuration file

# basic configuration

config setup
    # strictcrlpolicy&#x3D;yes
    uniqueids &#x3D; never

# Add connections here.
conn %default
    left&#x3D;%defaultroute
    ikelifetime&#x3D;60m
    keylife&#x3D;20m
    rekeymargin&#x3D;3m
    keyingtries&#x3D;1
    keyexchange&#x3D;ikev2  # 协议是ikev2
    authby&#x3D;secret  # 验证方式是密码，也可以用证书，证书配置起来比较麻烦，需要选用iphone支持的签名方法(懒得下证书了-，-#)

conn iOS_psk
    leftsubnet&#x3D;0.0.0.0&#x2F;0
    leftfirewall&#x3D;yes
    right&#x3D;%any
    rightsourceip&#x3D;10.63.41.0&#x2F;24  # 分配的内网ip
    auto&#x3D;add</code></pre><p>编辑<code>/etc/ipsec.secrets</code></p>
<pre><code># This file holds shared secrets or RSA private keys for authentication.

# RSA private key for this host, authenticating it to any other host
# which knows the public part.  Suitable public keys, for ipsec.conf, DNS,
# or configuration of other implementations, can be extracted conveniently
# with &quot;ipsec showhostkey&quot;.

: PSK &quot;&lt;你的密码，尽量不要使用太奇怪的字符&gt;&quot;</code></pre><p>编辑<code>/etc/strongswan.conf</code>，其他参数没有测试过，但是dns是一定需要的，不然的话上不了网，不知其他网友有没有这种情况。</p>
<pre><code># strongswan.conf - strongSwan configuration file
#
# Refer to the strongswan.conf(5) manpage for details
#
# Configuration changes should be made in the included files

charon {
    load_modular &#x3D; yes
        duplicheck.enable &#x3D; no
        compress &#x3D; yes
        dns1 &#x3D; 8.8.8.8
        dns2 &#x3D; 8.8.4.4
    plugins {
        include strongswan.d&#x2F;charon&#x2F;*.conf
    }
}

include strongswan.d&#x2F;*.conf</code></pre><hr>
<p><strong>打开ip转发</strong></p>
<pre><code>sudo sed -s &#39;s&#x2F;^#net.ipv4.ip_forward&#x3D;1&#x2F;net.ipv4.ip_forward&#x3D;1&#x2F;g&#39; &#x2F;etc&#x2F;sysctl.conf -i
sudo sysctl -p</code></pre><hr>
<p><strong>配置iptable</strong></p>
<p>注意里面配置的ip，和上面ipsec.conf配置的ip一致，这个就不用多解释了。</p>
<p>这里我为了解决iptable重启之后会失效的问题，用了iptables-save这个工具，找不到这个工具的也可以找其他工具帮忙。</p>
<pre><code>#!&#x2F;bin&#x2F;bash
iptables -A INPUT -p udp --dport 500 -j ACCEPT
iptables -A INPUT -p udp --dport 4500 -j ACCEPT
iptables -t nat -A POSTROUTING -s 10.63.41.0&#x2F;24 -o eth0 -j MASQUERADE
iptables -A FORWARD -s 10.63.41.0&#x2F;24 -j ACCEPT

# 保存iptables配置
iptables-save &gt; &#x2F;etc&#x2F;iptables&#x2F;rules.v4</code></pre><hr>
<p>自此，服务器端的配置都已经完成，启动strongswan我们就可以使用iphone翻墙了</p>
<pre><code># 普通启动
ipsec start

# 输出日志的启动
ipsec start --nofork</code></pre><hr>
<p><strong>iPhone配置</strong></p>
<p>&lt;img width=&quot;402&quot; alt=&quot;4e11705d-1eec-4edc-929e-66861f25b70f&quot; src=&quot;https://user-images.githubusercontent.com/3870517/32063455-d1b00fb6-ba3c-11e7-92ec-8bf7d6340a4b.png&quot;&gt;</p>
<p>描述可以随便写，服务器和远程ID都填服务器的ip地址，或者域名。</p>
<p>密码就是上文所配置的那个密码。</p>
<p>然后点击完成，vpn的配置就完成了。</p>
<p><strong>配置文件</strong></p>
<p>但是直接配置账号密码，并不能让iphone在切换网络之后自动连接。要自动连接的话，需要使用配置文件。</p>
<p>配置文件的样例如下：</p>
<pre><code>&lt;!DOCTYPE plist PUBLIC &quot;-&#x2F;&#x2F;Apple&#x2F;&#x2F;DTD PLIST 1.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.apple.com&#x2F;DTDs&#x2F;PropertyList-1.0.dtd&quot;&gt;
&lt;!-- Read more: https:&#x2F;&#x2F;wiki.strongswan.org&#x2F;projects&#x2F;strongswan&#x2F;wiki&#x2F;AppleIKEv2Profile --&gt;
&lt;plist version&#x3D;&quot;1.0&quot;&gt;
    &lt;dict&gt;
        &lt;!-- Set the name to whatever you like, it is used in the profile list on the device --&gt;
        &lt;key&gt;PayloadDisplayName&lt;&#x2F;key&gt;
        &lt;string&gt;${PROFILE_NAME}&lt;&#x2F;string&gt;
        &lt;!-- This is a reverse-DNS style unique identifier used to detect duplicate profiles --&gt;
        &lt;key&gt;PayloadIdentifier&lt;&#x2F;key&gt;
        &lt;string&gt;${PROFILE_IDENTIFIER}&lt;&#x2F;string&gt;
        &lt;!-- A globally unique identifier, use uuidgen on Linux&#x2F;Mac OS X to generate it --&gt;
        &lt;key&gt;PayloadUUID&lt;&#x2F;key&gt;
        &lt;string&gt;${PROFILE_UUID}&lt;&#x2F;string&gt;
        &lt;key&gt;PayloadType&lt;&#x2F;key&gt;
        &lt;string&gt;Configuration&lt;&#x2F;string&gt;
        &lt;key&gt;PayloadVersion&lt;&#x2F;key&gt;
        &lt;integer&gt;1&lt;&#x2F;integer&gt;
        &lt;key&gt;PayloadContent&lt;&#x2F;key&gt;
        &lt;array&gt;
            &lt;!-- It is possible to add multiple VPN payloads with different identifiers&#x2F;UUIDs and names --&gt;
            &lt;dict&gt;
                &lt;!-- This is an extension of the identifier given above --&gt;
                &lt;key&gt;PayloadIdentifier&lt;&#x2F;key&gt;
                &lt;string&gt;${CONN_IDENTIFIER}&lt;&#x2F;string&gt;
                &lt;!-- A globally unique identifier for this payload --&gt;
                &lt;key&gt;PayloadUUID&lt;&#x2F;key&gt;
                &lt;string&gt;${CONN_UUID}&lt;&#x2F;string&gt;
                &lt;key&gt;PayloadType&lt;&#x2F;key&gt;
                &lt;string&gt;com.apple.vpn.managed&lt;&#x2F;string&gt;
                &lt;key&gt;PayloadVersion&lt;&#x2F;key&gt;
                &lt;integer&gt;1&lt;&#x2F;integer&gt;
                &lt;!-- This is the name of the VPN connection as seen in the VPN application later --&gt;
                &lt;key&gt;UserDefinedName&lt;&#x2F;key&gt;
                &lt;string&gt;${CONN_NAME}&lt;&#x2F;string&gt;
                &lt;key&gt;VPNType&lt;&#x2F;key&gt;
                &lt;string&gt;IKEv2&lt;&#x2F;string&gt;
                &lt;key&gt;IKEv2&lt;&#x2F;key&gt;
                &lt;dict&gt;
                    &lt;!-- Hostname or IP address of the VPN server --&gt;
                    &lt;key&gt;RemoteAddress&lt;&#x2F;key&gt;
                    &lt;string&gt;${CONN_HOST}&lt;&#x2F;string&gt;
                    &lt;!-- Remote identity, can be a FQDN, a userFQDN, an IP or (theoretically) a certificate&#39;s subject DN. Can&#39;t be empty.
                     IMPORTANT: DNs are currently not handled correctly, they are always sent as identities of type FQDN --&gt;
                    &lt;key&gt;RemoteIdentifier&lt;&#x2F;key&gt;
                    &lt;string&gt;${CONN_REMOTE_IDENTIFIER}&lt;&#x2F;string&gt;
                    &lt;!-- Local IKE identity, same restrictions as above. If it is empty the client&#39;s IP address will be used --&gt;
                    &lt;key&gt;LocalIdentifier&lt;&#x2F;key&gt;
                    &lt;string&gt;&lt;&#x2F;string&gt;
                    &lt;!--
                    OnDemand references:
                    https:&#x2F;&#x2F;developer.apple.com&#x2F;library&#x2F;mac&#x2F;featuredarticles&#x2F;iPhoneConfigurationProfileRef&#x2F;Introduction&#x2F;Introduction.html
                    --&gt;
                    &lt;key&gt;OnDemandEnabled&lt;&#x2F;key&gt;
                    &lt;integer&gt;1&lt;&#x2F;integer&gt;
                    &lt;key&gt;OnDemandRules&lt;&#x2F;key&gt;
                    &lt;array&gt;
                        &lt;dict&gt;
                            &lt;key&gt;Action&lt;&#x2F;key&gt;
                            &lt;string&gt;Connect&lt;&#x2F;string&gt;
                        &lt;&#x2F;dict&gt;
                    &lt;&#x2F;array&gt;
                    &lt;!-- The server is authenticated using a certificate --&gt;
                    &lt;key&gt;AuthenticationMethod&lt;&#x2F;key&gt;
                    &lt;string&gt;SharedSecret&lt;&#x2F;string&gt;
                    &lt;key&gt;SharedSecret&lt;&#x2F;key&gt;
                    &lt;string&gt;${CONN_SHARED_SECRET}&lt;&#x2F;string&gt;
                    &lt;!-- The client uses EAP to authenticate --&gt;
                    &lt;key&gt;ExtendedAuthEnabled&lt;&#x2F;key&gt;
                    &lt;integer&gt;0&lt;&#x2F;integer&gt;
                &lt;&#x2F;dict&gt;
            &lt;&#x2F;dict&gt;
        &lt;&#x2F;array&gt;
    &lt;&#x2F;dict&gt;
&lt;&#x2F;plist&gt;</code></pre><p> 里面有不少东西需要自己替换成自己的配置。所以最后我干脆写了一个脚本，用来配置这个东西。</p>
<p>配置一共有3个文件</p>
<pre><code>config.sh  # 用于配置环境，并生成iphone使用的.mobileconfig文件
generate-mobileconfig.sh  # iphone使用的.mobileconfig文件的模板，config.sh会调用这个脚本
iptables.sh  # 配置iptables的脚本</code></pre><p><em>config.sh</em></p>
<pre><code>#!&#x2F;bin&#x2F;bash

HOST&#x3D;&quot;&lt;这里写服务器的ip，或者域名&gt;&quot;
export HOST&#x3D;$HOST

ipsec pki --gen --outform pem &gt; caKey.pem
ipsec pki --self --in caKey.pem --dn &quot;C&#x3D;CN, O&#x3D;myNetwork, CN&#x3D;myCA&quot; --ca --outform pem &gt; caCert.pem

ipsec pki --gen --outform pem &gt; serverKey.pem
ipsec pki --pub --in serverKey.pem | ipsec pki --issue --cacert caCert.pem --cakey caKey.pem --dn &quot;C&#x3D;CN, O&#x3D;myNetwork, CN&#x3D;$host&quot; --san&#x3D;&quot;$host&quot; --flag serverAuth --flag ikeIntermediate --outform pem &gt; serverCert.pem

ipsec pki --gen --outform pem &gt; clientKey.pem
ipsec pki --pub --in clientKey.pem | ipsec pki --issue --cacert caCert.pem --cakey caKey.pem --dn &quot;C&#x3D;CN, O&#x3D;strongSwan, CN&#x3D;iphoneClient&quot; --outform pem &gt; clientCert.pem

openssl pkcs12 -export -inkey clientKey.pem -in clientCert.pem -name &quot;iphoneClient&quot; -certfile caCert.pem -caname &quot;myCA&quot; -out clientCert.p12

sudo mv caCert.pem &#x2F;etc&#x2F;ipsec.d&#x2F;cacerts&#x2F;
sudo mv serverCert.pem &#x2F;etc&#x2F;ipsec.d&#x2F;certs&#x2F;
sudo mv caKey.pem &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;
sudo mv serverKey.pem &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;
sudo mv clientKey.pem &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;

sudo cp ipsec.conf &#x2F;etc&#x2F;
sudo cp ipsec.secrets &#x2F;etc&#x2F;
sudo cp strongswan.conf &#x2F;etc&#x2F;

sudo sed -s &#39;s&#x2F;^#net.ipv4.ip_forward&#x3D;1&#x2F;net.ipv4.ip_forward&#x3D;1&#x2F;g&#39; &#x2F;etc&#x2F;sysctl.conf -i
sudo sysctl -p

.&#x2F;generate-mobileconfig.sh &gt; $HOST.mobileconfig</code></pre><p><em>iptables.sh</em></p>
<pre><code>#!&#x2F;bin&#x2F;bash
iptables -A INPUT -p udp --dport 500 -j ACCEPT
iptables -A INPUT -p udp --dport 4500 -j ACCEPT
iptables -t nat -A POSTROUTING -s 10.63.41.0&#x2F;24 -o eth0 -j MASQUERADE
iptables -A FORWARD -s 10.63.41.0&#x2F;24 -j ACCEPT
iptables-save &gt; &#x2F;etc&#x2F;iptables&#x2F;rules.v4</code></pre><p><em>generate-mobileconfig.sh</em></p>
<pre><code>#!&#x2F;bin&#x2F;bash

# TODO: add regenerate shared secret option

# In normal cases, you will only need to pass the HOST of your server.
#[ &quot;no${HOST}&quot; &#x3D; &quot;no&quot; ] &amp;&amp; echo &quot;\$HOST environment variable required.&quot; &amp;&amp; exit 1
: ${PROFILE_NAME&#x3D;&quot;${HOST} Profile&quot;}
: ${PROFILE_IDENTIFIER&#x3D;$(echo -n &quot;${HOST}.&quot; | tac -s. | sed &#39;s&#x2F;\.$&#x2F;&#x2F;g&#39;)}
: ${PROFILE_UUID&#x3D;$(hostname)}
# These variable, especially CONN_UUID, are bind to per username,
# which currently, all users share the same secrets and configurations.
: ${CONN_NAME&#x3D;&quot;${HOST}&quot;}
: ${CONN_IDENTIFIER&#x3D;&quot;${PROFILE_IDENTIFIER}.shared-configuration&quot;}
: ${CONN_UUID&#x3D;$(uuidgen)}
: ${CONN_HOST&#x3D;${HOST}}
: ${CONN_REMOTE_IDENTIFIER&#x3D;${HOST}}
CONN_SHARED_SECRET&#x3D;$(grep &quot;PSK&quot; &#x2F;etc&#x2F;ipsec.secrets | sed &#39;s&#x2F;.*&quot;\(.*\)&quot;&#x2F;\1&#x2F;g&#39;)

cat &lt;&lt;EOF
&lt;!DOCTYPE plist PUBLIC &quot;-&#x2F;&#x2F;Apple&#x2F;&#x2F;DTD PLIST 1.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.apple.com&#x2F;DTDs&#x2F;PropertyList-1.0.dtd&quot;&gt;
&lt;!-- Read more: https:&#x2F;&#x2F;wiki.strongswan.org&#x2F;projects&#x2F;strongswan&#x2F;wiki&#x2F;AppleIKEv2Profile --&gt;
&lt;plist version&#x3D;&quot;1.0&quot;&gt;
    &lt;dict&gt;
        &lt;!-- Set the name to whatever you like, it is used in the profile list on the device --&gt;
        &lt;key&gt;PayloadDisplayName&lt;&#x2F;key&gt;
        &lt;string&gt;${PROFILE_NAME}&lt;&#x2F;string&gt;
        &lt;!-- This is a reverse-DNS style unique identifier used to detect duplicate profiles --&gt;
        &lt;key&gt;PayloadIdentifier&lt;&#x2F;key&gt;
        &lt;string&gt;${PROFILE_IDENTIFIER}&lt;&#x2F;string&gt;
        &lt;!-- A globally unique identifier, use uuidgen on Linux&#x2F;Mac OS X to generate it --&gt;
        &lt;key&gt;PayloadUUID&lt;&#x2F;key&gt;
        &lt;string&gt;${PROFILE_UUID}&lt;&#x2F;string&gt;
        &lt;key&gt;PayloadType&lt;&#x2F;key&gt;
        &lt;string&gt;Configuration&lt;&#x2F;string&gt;
        &lt;key&gt;PayloadVersion&lt;&#x2F;key&gt;
        &lt;integer&gt;1&lt;&#x2F;integer&gt;
        &lt;key&gt;PayloadContent&lt;&#x2F;key&gt;
        &lt;array&gt;
            &lt;!-- It is possible to add multiple VPN payloads with different identifiers&#x2F;UUIDs and names --&gt;
            &lt;dict&gt;
                &lt;!-- This is an extension of the identifier given above --&gt;
                &lt;key&gt;PayloadIdentifier&lt;&#x2F;key&gt;
                &lt;string&gt;${CONN_IDENTIFIER}&lt;&#x2F;string&gt;
                &lt;!-- A globally unique identifier for this payload --&gt;
                &lt;key&gt;PayloadUUID&lt;&#x2F;key&gt;
                &lt;string&gt;${CONN_UUID}&lt;&#x2F;string&gt;
                &lt;key&gt;PayloadType&lt;&#x2F;key&gt;
                &lt;string&gt;com.apple.vpn.managed&lt;&#x2F;string&gt;
                &lt;key&gt;PayloadVersion&lt;&#x2F;key&gt;
                &lt;integer&gt;1&lt;&#x2F;integer&gt;
                &lt;!-- This is the name of the VPN connection as seen in the VPN application later --&gt;
                &lt;key&gt;UserDefinedName&lt;&#x2F;key&gt;
                &lt;string&gt;${CONN_NAME}&lt;&#x2F;string&gt;
                &lt;key&gt;VPNType&lt;&#x2F;key&gt;
                &lt;string&gt;IKEv2&lt;&#x2F;string&gt;
                &lt;key&gt;IKEv2&lt;&#x2F;key&gt;
                &lt;dict&gt;
                    &lt;!-- Hostname or IP address of the VPN server --&gt;
                    &lt;key&gt;RemoteAddress&lt;&#x2F;key&gt;
                    &lt;string&gt;${CONN_HOST}&lt;&#x2F;string&gt;
                    &lt;!-- Remote identity, can be a FQDN, a userFQDN, an IP or (theoretically) a certificate&#39;s subject DN. Can&#39;t be empty.
                     IMPORTANT: DNs are currently not handled correctly, they are always sent as identities of type FQDN --&gt;
                    &lt;key&gt;RemoteIdentifier&lt;&#x2F;key&gt;
                    &lt;string&gt;${CONN_REMOTE_IDENTIFIER}&lt;&#x2F;string&gt;
                    &lt;!-- Local IKE identity, same restrictions as above. If it is empty the client&#39;s IP address will be used --&gt;
                    &lt;key&gt;LocalIdentifier&lt;&#x2F;key&gt;
                    &lt;string&gt;&lt;&#x2F;string&gt;
                    &lt;!--
                    OnDemand references:
                    https:&#x2F;&#x2F;developer.apple.com&#x2F;library&#x2F;mac&#x2F;featuredarticles&#x2F;iPhoneConfigurationProfileRef&#x2F;Introduction&#x2F;Introduction.html
                    --&gt;
                    &lt;key&gt;OnDemandEnabled&lt;&#x2F;key&gt;
                    &lt;integer&gt;1&lt;&#x2F;integer&gt;
                    &lt;key&gt;OnDemandRules&lt;&#x2F;key&gt;
                    &lt;array&gt;
                        &lt;dict&gt;
                            &lt;key&gt;Action&lt;&#x2F;key&gt;
                            &lt;string&gt;Connect&lt;&#x2F;string&gt;
                        &lt;&#x2F;dict&gt;
                    &lt;&#x2F;array&gt;
                    &lt;!-- The server is authenticated using a certificate --&gt;
                    &lt;key&gt;AuthenticationMethod&lt;&#x2F;key&gt;
                    &lt;string&gt;SharedSecret&lt;&#x2F;string&gt;
                    &lt;key&gt;SharedSecret&lt;&#x2F;key&gt;
                    &lt;string&gt;${CONN_SHARED_SECRET}&lt;&#x2F;string&gt;
                    &lt;!-- The client uses EAP to authenticate --&gt;
                    &lt;key&gt;ExtendedAuthEnabled&lt;&#x2F;key&gt;
                    &lt;integer&gt;0&lt;&#x2F;integer&gt;
                &lt;&#x2F;dict&gt;
            &lt;&#x2F;dict&gt;
        &lt;&#x2F;array&gt;
    &lt;&#x2F;dict&gt;
&lt;&#x2F;plist&gt;
EOF</code></pre><p>最后整个配置也只有我自己试用过是可以的。如果发现用不了，请自己上网找怎么解决。</p>
<p>生成了<code>.mobileconfig</code>文件后，使用<code>python -m SimpleHTTPServer</code>命令运行一个web服务，然后在iphone上用Safari打开这个文件，安装配置。然后你就有了一个可以自动连接的ikev2配置。</p>
<hr>
<p>后记：iphone上我只使用过openvpn和ikev2个人感觉ikev2十分稳定，到目前为止没有出现过被block的情况。出门打po面基都十分方便，用instagram装逼也是妥妥的。</p>
<p>PS：我是蓝...</p>
]]></description><link>http:/ruibo.me/posts/268818423.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268818423.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:25:32 GMT</pubDate></item><item><title><![CDATA[用户数统计运算]]></title><description><![CDATA[<p>&lt;!-- datetime: 2015-12-11 22:11:34 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>用户数统计基本上有三种做法。</p>
<ol>
<li>使用一个Set来直接记录用户数</li>
<li>自己实现类似BloomFilter的方法</li>
<li>HyperLogLog</li>
</ol>
<h4>HyperLogLog</h4><p>HyperLogLog(HLL)作为比较新的一种方案，比起BloomFilter，错误可以比较稳定的维持在一个较低的水平。绝大多数情况下误差不回超过5%，平均错误在1%以内，对于超大量的用户量级维护十分友好。</p>
<p>Redis里原生提供对HyperLogLog的支持，官方说明12K的key可以存放2^64的uniqueid</p>
<p>当然，如果要保证毫无错误的话，还是直接使用Set来保存是最好。</p>
<p>关于HyperLogLog的文章可以参考 <a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html">http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html</a> 或者自行Google</p>
<h4>容斥原理</h4><p>我们会遇到这样一种情况——我们有许多的用户数据，包用户的使用的手机品牌型号，地理位置，运营商等信息。</p>
<p>这时我们想要计算每一天，我们在各个省市有多少用户、各个品牌上有多少用户、联通有多少用户，移动有多少用户、有多少用户使用WIFI接入我们的产品，有多少用户使用3G接入我们的产品。</p>
<p>甚至，我们还想知道，广州小米有多少用户？，浙江三星有多少用户？；又或者在深圳使用小米的联通用户有多少？北京使用iPhone6s的移动用户里4G的用户有多少？</p>
<p>看到这么多的排列组合，即使手头握着HyperLogLog这样的神器，也开始不淡定了呢。</p>
<p>于是我们想到，能不能记录下每天广州的用户数HLL以及使用iPhone的用户数的HLL，然后将两个HLL做一个Intersection，就可以近似的算出每天广州的iPhone用户有多少？</p>
<p>想法是挺好的，但是HyperLogLog并不能支持简单的Intersection。单纯的使用异或运算运算是无法得出正确结果的。</p>
<p>幸运的是，HyperLogLog作为一种数据集合，是可以直接使用集合公式进行运算的。</p>
<p>以下使用Redis自带的HyperLogLog为基础进行计算，Redis中PFCOUNT可以合并计算多个HLL的结果，结果相当于计算多个HLL集合的并集。</p>
<p>即：<code>PFCOUNT k1 k2 k3 == k1 ∪ k2 ∪ k3</code></p>
<p>根据集合运算中的容斥原理（请参见高中课本或<a href="https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle），集合A跟B的交集等于A跟B的和减去A跟B的交集。">https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle），集合A跟B的交集等于A跟B的和减去A跟B的交集。</a></p>
<p>即：<code>|A∪B|=|A|+|B|-|A∩B|</code>，所以可得：<code>|A∩B|=|A|+|B|-|A∪B|</code>。</p>
<p>即假设，集合A有10个元素，集合B有6个元素，A与B的并集有12个元素，那么可以算得A与B的交集必然有10+6-12=4个元素。</p>
<p>回到Redis，根据上面所列公式，计算<code>k1∩k2</code>的方法为：<code>PFCOUNT k1 + PFCOUNT k2 - PFCOUNT k1 k2</code></p>
<p>3个元素的情况下，<code>|A∪B∪C|=|A|+|B|+|C|-|A∩B|-|A∩C|-|B∩C|+|A∩B∩C|</code></p>
<p>所以计算<code>k1∩k2∩k3</code>的方法也可以递归得出，同理更高维的情况。</p>
<p>n个维度：</p>
<p><img src="../assets/aklshdfqewjfhqef.png" alt=""></p>
<p>习题 _(•̀ω•́ 」∠)_：</p>
<pre><code>求a,b,c,d,e,f六个字母的全排列中不允许出现ace和df图象的排列数。</code></pre><h4>MinHash</h4><p>上面的习题做不出来的话，请看这里∠( ᐛ 」∠)＿</p>
<p>MinHash原本是搜索引擎用来快速估算两个网页的相似度而研究出来的一种算法，她可以用来快速计算两个集合的相似程度。</p>
<p>介绍见此 <a href="https://en.wikipedia.org/wiki/MinHash">https://en.wikipedia.org/wiki/MinHash</a></p>
<p>MinHash会对数据进行Hash并得到一个较为简短的摘要，对内存开销十分友好，这点与HLL类似。</p>
<p>MinHash度量结果叫做Jaccard similarity，记做<code>J</code>，其中有公式：</p>
<p><img src="../assets/23wieyroiwr.png" alt=""></p>
<p>多维的情况下，可以表示为：</p>
<p><img src="../assets/akdgfjkaehe.png" alt=""></p>
<p>由上面两个公式可以看到，求交集的方法已经呼之欲出。</p>
<h4>最后</h4><p>值得注意的是，因为HyperLogLog本身并不是100%无差统计，每个HLL的数据集里都可能会多多少少有一些误差，在做容斥运算的时候会导致这些误差增大。最坏的情况下，误差可能会达到30%多，且在做减法运算时，可能会出现负数的情况。</p>
<h4>参考：</h4><p><a href="http://tech.adroll.com/blog/data/2013/07/10/hll-minhash.html">http://tech.adroll.com/blog/data/2013/07/10/hll-minhash.html</a></p>
<p><a href="http://research.neustar.biz/2012/12/17/hll-intersections-2/">http://research.neustar.biz/2012/12/17/hll-intersections-2/</a></p>
]]></description><link>http:/ruibo.me/posts/268816800.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268816800.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:27:03 GMT</pubDate></item><item><title><![CDATA[小水管支撑大流量-Graphite的IO优化实践]]></title><description><![CDATA[<p>&lt;!-- datetime: 2015-05-06 02:01:23 --&gt;
</p>
<p>在实际的项目开发中，用于做运行时监控的工具往往无法分配到高配置的服务器，但可能又要接受100K以上的Metrics发送，特别是在流量暴涨的时候还要能Hold得主。要怎么解决这个问题呢？</p>
<p>&lt;!-- more --&gt;
</p>
<p>首先推荐两片文章：<br><a href="http://calvin1978.blogcn.com/articles/graphite.html">http://calvin1978.blogcn.com/articles/graphite.html</a>网上各种转载，我也不知道那篇才是原文<br><a href="https://kevinmccarthy.org/blog/2013/07/18/10-things-i-learned-deploying-graphite/">https://kevinmccarthy.org/blog/2013/07/18/10-things-i-learned-deploying-graphite/</a>绝逼推荐，尤其是讲interval的那节    </p>
<p>做我们这行的，什么不多，就是请求特别多，什么不大，就是流量特别大-，-#</p>
<p>首先我们的Graphite只有一台小机器，用于投放的机器有12台。并且放Graphite的这台小机器还兼顾着放许多的三方监控，放sentry错误采集等等周边产品。你想想一个异性恋小正太站在12个彪悍凶残的兄贵中间，是一种什么样的体验-，-#</p>
<p>好了本来即便是如此，Graphite还是能hold住的，只是有时会比较卡。最近ifstat的时候发现，Graphite所在的服务器网络IO经常跑满，磁盘IOPS常年保持在500+。虽然没有什么问题，但总归是一种浪费，也是一个隐患。</p>
<p>好吧，其实我们有两台Graphite，一台放在AWSCN上，常年IOPS在500+，不过没关系，我们和成一台来描述O(∩_∩)O~</p>
<p>总之问题就是，网络带宽占用过高，磁盘IO过高，CPU过高。</p>
<p>Graphite有两种接受Metrics的方式，一种是用String接收，比如接受Statsd的传参。另一种是用Pakcle接收Metrics，使用Pakcle可以Batch发送一批Metrics(其实string也是可以的)，并且压缩效果比较好。</p>
<p>我们在AWSCN上申请的机器IOPS最高是300，500+的IOPS已经明显超出上限了。在carbon.conf中有一个选项叫<code>MAX_UPDATES_PER_SECOND</code>，调到100，重启，再看，IOPS立马掉下来了。目前没有发现副作用，副作用不明。</p>
<p>我一开始传Metrics时的做法，是在各台投放机上直接往Graphite机发送数据给到Statsd，Statsd汇总完转发给carbon-cache。那么既然网络带宽高，我们就得想办法减少数据的传输。</p>
<p>我去粗略翻阅了一下Statsd的文档，发现这东西就是用来聚合Metrics的，聚合之后，再将数据一并发送给carbon-cache，那假如我再各个投放机上装Statsd，是不是可以减少发送的频次从而降低网络IO。有了这个Idea后就去投放机上装了Statsd，并且把发送的地址全部改为了127.0.0.1。然后由每台投放机的Statsd将数据汇总后发到Graphite机上。这样一套下来，发现没！有！用！-，-#</p>
<p>这时重新去细细翻阅了一下Statsd，其实Statsd的主旨是讲不同的Metrics用相同的一种格式来表达。而并不是通过合并相同的Metrics来减少Metrics的数量，这样当然无法减少网络IO。</p>
<p>这时我又看到了carbon-aggregation。官方说这东西可以挂在carbon-cache之前，用来做一次预先的聚合，可以显著降低IO。</p>
<p>这里有几个比较关键的配置点：</p>
<pre><code>[aggregation]
AGGREGATION_RULES &#x3D; &#x2F;etc&#x2F;carbon&#x2F;aggregation-rules.conf
DESTINATIONS &#x3D; 192.168.1.20:2004
MAX_QUEUE_SIZE &#x3D; 20000</code></pre><p>AGGREGATION_RULES 配置聚合的方式，聚合的方式在官方github的example中有比较详细的注释，我这里就不再多重复一次了，在aggregation-rules.conf里，我直接增加了一行<code>stats.counters.logc.&lt;metrics&gt; (5) = sum
stats.counters.logc.&lt;&lt;metrics&gt;&gt;</code> 表示凡是<code>stats.counters.logc.</code>开头的所有Metrics，都在不改变Metrics名称的前提下，sum每5秒的数据成一条Metrics，然后发送到<code>DESTINATIONS</code>里。</p>
<p>DESTINATIONS是转发的地址，aggregation聚合后的数据会转发到这里，注意会有多个<code>DESTINATIONS</code>标签，我们使用的是[aggregation]下面的那个。</p>
<p>如果我们的Metrics过多，carbon-cache消耗不完，就会导致carbon-aggregation这边堆积大量的Metrics，一旦超过上限，carbon-aggregation就会直接丢弃掉，carbon-aggregation丢弃Metrics的时候，会写入一条日志。遇到这种情况时，我们需要设置MAX_QUEUE_SIZE。将MAX_QUEUE_SIZE适当调大一些，让carbon有足够的空间去完成他的工作。大家可以自行根据自己的情况调整这个参数。</p>
<p>仔细看aggregation的example的人会发现，如果在aggregation-rules改变了Metrics，可能会导致carbon-aggregaion发送double的数据。如<code>statsd.counters.logc.x.servers (60) = sum
statsd.counters.logc.&lt;type&gt;.servers</code>
这样一条Metrics，carbon-aggregation会发送<code>statsd.counter.logc.&lt;type&gt;.servers</code>的数据，也会发送聚合后的<code>statsd.counters.logc.x.servers</code>。</p>
<p>这是我们的流程已经改为：在每台投放机上部署Statsd和carbon-aggregation，Statsd采集本机的Metrics给到本机的carbon-aggregaion，经过聚合后在发送给Graphite机的carbon-cache。</p>
<p>到这里，再运行ifstat查看网络流量，已经变为了之前的1/10。</p>
<p>过了两天后，居然TMD丢数据。。。</p>
<p>解决方法是，在Graphite机器上也运行carbon-aggregation，配置与其他机器完全一致，只是DESTINATIONS为carbon-cache的地址。
然后投放机的carbon-aggregation的DESTINATIONS改为Graphite机得carbon-aggregation的Pakcle接收地址。
即将数据聚合完后，转发到Graphite机的carbon-aggregation上，再聚合一次，然后统一转发给carbon-cache，这样就可以解决Metrics丢失的问题。</p>
<p>Metrics丢失的原因没有确定，推测可能是有carbon-cache的BufferedPool引起的。不要问我是怎么知道的 T，T</p>
<p>carbon-aggregation的聚合时间不宜太长，否则Graphite图表上会出现大量丑陋的锯齿。我目前聚合的频率为5s一次，在Graphite上看不出任何锯齿。</p>
<p>对于多久聚合一次，其实并不需要太过纠结，以为一旦使用了carbon-aggregation，Metrics的发送数量就和请求并发量没有直接关联了，转而与统计的项目直接挂钩，因为不论有多大量的请求过来，每一个统计项最后都会被聚合为5秒一次的Metrics。</p>
<p>至此，在使用ifstat查看，Graphite机平均网络带宽占用已经不足100k了，波峰也在300k以内。而因为每台投放机都已经自己把最大量的Metrics聚合起来，所以Graphite机上的carbon-cache的cpu占用也是极低。聚合后的数据，对于Graphite的渲染似乎也是有些帮助的。</p>
<p>最后，虽然我的Metrics只有10w+/s，但是我相信在这样的结构下，即使是100w+的Metrics也是可以Hold住的。阿门~~</p>
]]></description><link>http:/ruibo.me/posts/268814269.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268814269.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:26:00 GMT</pubDate></item><item><title><![CDATA[使用Logrotated清理日志文件]]></title><description><![CDATA[<p>&lt;!--datetime: 2015-02-22 22:23:44 --&gt;
</p>
<p>在Linux环境下，使用Logrotated工具定期自动清理历史的运行日志。
&lt;!-- more --&gt;</p>
<p>题外话：Logrotated是依靠cron来执行的，他的脚本放在/etc/cron.daily下。其实许多定时执行的功能如apc cache清理过期数据等等，也是依赖cron执行的。</p>
<p>首先，要比较全面的了解Logrotated的使用，推荐阅读<a href="http://linux.vbird.org/linux_basic/0570syslog.php#rotate_config">鸟哥的私房菜Logrotated部分</a>。</p>
<p>我这里讲一些我自己使用的情况。</p>
<pre><code>&#x2F;var&#x2F;log&#x2F;php5-fpm.log {
    rotate 12   #保留12份，也就是会有php5-fpm.log.1.gz ~ php5-fpm.log.12.gz
    maxage 12   #包括12天
    weekly  #每周执行一次
    missingok   #没有找到log文件也OK
    notifempty  #如果log文件是空的，就不做rotate
    compress    #gzip压缩，所以文件会是 php5-fpm.log.1.gz
    delaycompress   #和compress一起使用，转储的日志文件到下一次转储时才压缩
    minsize 10M     #小于1M的log文件不做rotate
    postrotate  #rotate后执行里面得命令
        invoke-rc.d php5-fpm reopen-logs &gt; &#x2F;dev&#x2F;null
    endscript
}</code></pre><p>rotate和maxage都是控制日志保留的，不过前者是以个数为单位，后者以天数为单位。</p>
<p>具体logrotated会在什么时候运行，这个需要看cron.daily会在什么时候运行，在<code>/etc/crontab</code>里可以看到运行时间。</p>
<p>一般自己写的程序，我习惯使用两种logrotated的方式。</p>
<p>如果log本身并没有特别作用，只是一些例行的log信息，那么可以直接删掉，一般会使用</p>
<pre><code>weekly
missingok
rotate 0    #完全不需要保存数据，直接删掉
su root audio</code></pre><p>如果是需要保留一段时间以备查阅的，一般使用</p>
<pre><code>daily
compress
missingok
rotate 14
su root audio</code></pre><p>是否需要compress要看log文件的大小和具体的业务，保存几天也视情况而定。
不过一般情况下我会选择压缩+保存2周时间。</p>
<p>su root audio这个命令是用于解决权限问题的，一般不会加，
但是如果你发现你的log文件因为权限问题无法正确的归档时，
可以增加该命令。
<a href="https://linuxslut.net/logrotate-parent-directory-has-insecure-permissions/">[参考]</a></p>
<p>编辑好你的rotate脚本后，保存到/etc/logrotate.d下，最好对于每一个日志，
都编辑一个与之对应的logrotated脚本。当然也可以把多个脚本放在一个文件里。</p>
<p>保存好之后不需要重启什么服务，下次日志归档时就会自动生效了。</p>
<p>当然，如果你不知道你的脚本写的好不好的话，也可以手动运行logrotated来执行指定脚本。</p>
<pre><code>sudo logrotate -vf &lt;你的logrotated脚本路径&gt;</code></pre><p>使用-v会打印logrotated的执行过程，如果有报错可以直接看到。</p>
<p>在使用的过程中，我们有时会发现log文件无法归档，或者log文件归档后，没有再生成新的log文件。</p>
<p>比如说，我现在有一个log文件叫做celery.log，并不断有日志往这个文件里写入。<br>在归档后，文件名变成了celery.log.1.gz。<br>这时程序理应生成一个新的celery.log并往里面写数据，
但是却不见celery.log出现。<br>这是因为程序一直持有老的celery.log的fd，在老的celery.log变成celery.log.1.gz后，程序并没有释放fd，所以就不会生成新的celery.log。</p>
<p>解决这个问题的方法要看具体案例，比如说php5-fpm就使用上文的postrotate语法段来解决。<br>如果是我们自己写的python程序，使用logging的FileHandler时也会出现这个问题，可以改用WatchedFileHandler解决问题。<br>当然使用RotatingFileHandler/TimedRotatingFileHandler来自己做rotate也是可以的。</p>
]]></description><link>http:/ruibo.me/posts/268812978.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268812978.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:27:46 GMT</pubDate></item><item><title><![CDATA[ubuntu上安装和配置graphite+statsd]]></title><description><![CDATA[<p>&lt;!-- datetime: 2015-02-21 22:09:11 --&gt;
</p>
<p>在Ubuntu上配置Graphite加Statsd来采集程序运行状态。
&lt;!-- more --&gt;</p>
<p>常见的开源监控工具有nagios, ganglia, graphite等等，各有各的特点。
第一次听说graphite是在nsq的文档上，他声称官方支持使用graphite+statsd来统计metric。
于是乎便捣鼓了一下。</p>
<p>ubuntu12.04和ubuntu14.04上的安装方法略有不同，总体来说我更喜欢ubuntu14.04的使用方法。</p>
<hr>
<p><strong>ubuntu12.04</strong></p>
<p>首先，安装各种依赖。graphite会安装到/opt/graphite，不伦有没有使用virtualenv，不过我们依然建议使用virtualenv。
graphite-web的各种依赖可以看<a href="https://github.com/graphite-project/graphite-web/blob/master/requirements.txt">这里</a>。  </p>
<p>这里我没有贴出所需要得依赖，因为我自己也忘记了。不想看依赖的可以像我一样，装到哪里发现不行的时候再去补充依赖。</p>
<p>需要注意的是django和django-tagging两个依赖，django版本不兼容也已经不是新鲜事了，如果发现django报错，可能是你的版本使用的不对。<br>我这里使用的是</p>
<pre><code>Django (1.6.1)
django-tagging (0.3.1)</code></pre><p>系统库也需要补充一些<code>sudo apt-get install libcairo2-dev</code>  </p>
<pre><code>pip install whisper
pip install carbon
pip install graphite-web</code></pre><p>这里介绍一下，whisper是一个文件数据库，用来存储采集到的metrics。你采集到的metrics会扔到carbon里，然后carbon会存储到whisper上。
graphite-web从whisper中拿出各种metrics来画图，画图用的库我们刚刚用apt-get安装了。<br>metrics是什么？比如说<code>stats.cpu.host-01.cpu-00</code>就是一个metrics，也是一个key，他的value是50，就表示host-01这台机器的cpu-00现在是50%的状态。<br>使用点号分割。但是我们不需要在意这些细节。</p>
<p>安装完后，你会发现/opt/graphite下多了很多东西，把/opt/graphite/conf下面的*.example的.example去掉cp到对应的目录即可。默认graphite会使用2003，2004端口。</p>
<p>启动carbon</p>
<pre><code>&#x2F;opt&#x2F;graphite&#x2F;bin&#x2F;carbon-cache.py start</code></pre><p>制造一些metrics发送</p>
<pre><code>vim &#x2F;etc&#x2F;hosts
127.0.0.1   graphite
python &#x2F;opt&#x2F;graphite&#x2F;examples&#x2F;example-client.py</code></pre><p>数据会存放在</p>
<pre><code>&#x2F;opt&#x2F;graphite&#x2F;storage&#x2F;whisper</code></pre><p>现在去看看，是不是有一些东西了。</p>
<p>同步数据库</p>
<pre><code>python &#x2F;opt&#x2F;graphite&#x2F;webapp&#x2F;graphite&#x2F;manage.py syncdb</code></pre><p>启动graphite-web</p>
<pre><code>python &#x2F;opt&#x2F;graphite&#x2F;webapp&#x2F;graphite&#x2F;manage.py runserver 0.0.0.0:12222</code></pre><p>打开浏览器 <a href="http://127.0.0.1:12222，看看是不是有了一些东西。">http://127.0.0.1:12222，看看是不是有了一些东西。</a></p>
<hr>
<p><strong>ubuntu14.04</strong></p>
<p>14.04的官方库里就有了graphite，可以直接安装。老夫装完后升级到14.04又重新用新的方法装一次-，-#</p>
<pre><code>sudo apt-get install graphite-web graphite-carbon</code></pre><p>apt-get安装后，配置文件在/etc/graphite/, /etc/carbon/下。<br>数据库放在/var/lib/graphite/下。<br>graphite的uwsgi文件在/usr/share/graphite-web下。<br>graphite的py文件在/usr/lib/python2.7/dist-packages/graphite<br>graphite的可执行文件为graphite-manage，可以直接运行<br>配置的example在/usr/share/doc下  </p>
<hr>
<p><strong>配置Graphite</strong></p>
<p>14.04的配置在/etc/graphite/local_settings.py里，ubuntu12.04在/opt里有对应的配置文件。<strong>往后的配置如无特别说明都以ubuntu14.04为主，因为我已经没有12.04的环境，找不回配置文件所在了。</strong></p>
<pre><code>SECRET_KEY &#x3D; &#39;yw7HIHFkW459HuswEUAZOn12NgQGx+z6\n&#39;</code></pre><p>可以使用python来生成</p>
<pre><code>import os
os.urandom(24).encode(&#39;base64&#39;)</code></pre><pre><code>#修改时区
TIME_ZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;
#打开RemoteUser身份认证（不知道干嘛用的）
USE_REMOTE_USER_AUTHENTICATION &#x3D; True</code></pre><p>配置好后同步数据库:</p>
<pre><code>#ubuntu14.04
sudo graphite-manage syncdb

#ubuntu12.04
python &#x2F;opt&#x2F;graphite&#x2F;webapp&#x2F;graphite&#x2F;manage.py syncdb</code></pre><p><strong>配置Carbon</strong></p>
<p>配置carbon开机启动（这项配置ubuntu12.04里没有）</p>
<pre><code>sudo vim &#x2F;etc&#x2F;default&#x2F;graphite-carbon

CARBON_CACHE_ENABLED&#x3D;true</code></pre><p>开启carbon的log rotation</p>
<pre><code>sudo vim &#x2F;etc&#x2F;carbon&#x2F;carbon.conf

ENABLE_LOGROTATION&#x3D;true</code></pre><p>配置storage schemas，在最上面添加一个新的schema</p>
<pre><code>sudo vim &#x2F;etc&#x2F;carbon&#x2F;storage-schemas.conf

[statsd]
pattern &#x3D; ^stats\.
retentions &#x3D; 15s:1d,1m:7d,15m:5y</code></pre><p>上面这段statsd的意思是：</p>
<ul>
<li>匹配以<code>stats.</code>为开头的所有metrics</li>
<li>每隔15s创建一个数据点，保存1d，请求最近1d的数据时使用这套数据</li>
<li>每隔1m创建一个数据点，保存7d，请求7d内得数据时使用这套数据</li>
<li>每隔15m创建一个数据点，保存5y，请求5y内得数据时使用这套数据</li>
</ul>
<p>修改数据聚合的方法aggregation method</p>
<pre><code>sudo vim &#x2F;etc&#x2F;carbon&#x2F;storage-aggregation.conf

#可以改为以下(网上抄的)
[min]
pattern &#x3D; \.min$
xFilesFactor &#x3D; 0.1
aggregationMethod &#x3D; min

[max]
pattern &#x3D; \.max$
xFilesFactor &#x3D; 0.1
aggregationMethod &#x3D; max

[count]
pattern &#x3D; \.count$
xFilesFactor &#x3D; 0
aggregationMethod &#x3D; sum

[lower]
pattern &#x3D; \.lower(_\d+)?$
xFilesFactor &#x3D; 0.1
aggregationMethod &#x3D; min

[upper]
pattern &#x3D; \.upper(_\d+)?$
xFilesFactor &#x3D; 0.1
aggregationMethod &#x3D; max

[sum]
pattern &#x3D; \.sum$
xFilesFactor &#x3D; 0
aggregationMethod &#x3D; sum

[gauges]
pattern &#x3D; ^.*\.gauges\..*
xFilesFactor &#x3D; 0
aggregationMethod &#x3D; last

[default_average]
pattern &#x3D; .*
xFilesFactor &#x3D; 0.5
aggregationMethod &#x3D; average</code></pre><p>其中的xFileFactor的值代表carbon做聚合的最小百分比值，根据你的具体情况修改。</p>
<p>启动carbon-cache</p>
<pre><code>sudo service carbon-cache start</code></pre><p><strong>Nginx配置</strong></p>
<pre><code>sudo vim &#x2F;etc&#x2F;nginx&#x2F;site-enabled&#x2F;graphite

upstream graphite {
    server unix:&#x2F;&#x2F;&#x2F;run&#x2F;uwsgi&#x2F;app&#x2F;graphite&#x2F;socket;
    keepalive 600;
}
server {
    listen 80;
    server_name XXXX;

    access_log off;
    error_log off;

    location &#x2F; {
        uwsgi_pass graphite;
        include uwsgi_params;
    }
}</code></pre><p><strong>Uwsgi配置</strong></p>
<pre><code>sudo vim &#x2F;etc&#x2F;uwsgi&#x2F;apps-available&#x2F;graphite.ini

[uwsgi]
processes &#x3D; 8
uid &#x3D; _graphite
gid &#x3D; _graphite
master &#x3D; True
chdir &#x3D; &#x2F;usr&#x2F;share&#x2F;graphite-web
wsgi-file &#x3D; graphite.wsgi
chmod-socket &#x3D; 666
enable-threads &#x3D; true</code></pre><p>启动服务</p>
<pre><code>sudo service carbon-cache restart
sudo service nginx restart
sudo service uwsgi restart</code></pre><p>打开浏览器 <a href="http://XXXX:80">http://XXXX:80</a></p>
<p><strong>安装statsd</strong></p>
<p>一般使用statsd来收集各种metrics。然后传给carbon，carbon再保存到whisper里。</p>
<p>安装所需要的依赖</p>
<pre><code>sudo apt-get install git nodejs devscripts debhelper</code></pre><p>下载并编译statsd</p>
<pre><code>mkdir statsd_build
cd statsd_build
git clone https:&#x2F;&#x2F;github.com&#x2F;etsy&#x2F;statsd.git
cd statsd
dpkg-buildpackage
cd ..
sudo dpkg -i *.deb</code></pre><p>关闭statsd的legacy namespacing</p>
<pre><code>sudo vim &#x2F;etc&#x2F;statsd&#x2F;localConfig.js

{
  graphitePort: 2003
, graphiteHost: &quot;localhost&quot;
, port: 8125
, graphite: {
    legacyNamespace: false
  }
}</code></pre><p>上文所配置的carbon配置，已经是按照statsd的格式来配置了，所以这里不再重复。</p>
<p>重启服务</p>
<pre><code>sudo service carbon-cache restart
sudo service statsd start</code></pre><p>访问graphite，可以看到statsd自身的一些记录。statsd有许多的client，大家可以自己上github上搜索自己语言使用的client。</p>
<p>需要删掉数据的时候，可以直接到whisper下删除对应的文件，当然要先停止掉carbon和statsd的服务。</p>
<p>到这里已经可以正常的看到图像了，如果不行，可以自己再去google。</p>
<p><strong>配置Dashboard</strong></p>
<p>Graphite的原装web实在是看不下去，当然人家主打的也不是这个界面，所以有许多扩展界面出现。可以参考<a href="http://dashboarddude.com/blog/2013/01/23/dashboards-for-graphite/">这里</a></p>
<p>豆瓣也出了一个<a href="https://github.com/douban/graph-index">graphite-index</a>，主要是配置方便。</p>
<p>用过kibana3的同学可以考虑使用<a href="https://github.com/torkelo/grafana">grafana</a>，该项目是一个基于kibana3的改版而来的Dashboard，配置方法和kibana3很相似。</p>
<p>这里我选择的是golang版的grafana <a href="https://github.com/jwilder/gofana">gofana</a>，为什么要使用这个呢，因为他配置起来很方便，可以直接使用docker来安装。而且使用起来也和kibana类似。</p>
<p>安装得方法在github上写得十分详细，我这里就不再重复。</p>
<p>这里要注意几点，kibana系列的Dashboard是自己用js绘图的，对于数据量很大的图表，绘制起来会很慢，原装的就不会有这个问题。我现在平均1s有1k+的metrics采集，查看7d的数据已经卡成狗，是有必要修改一下storage_schema的。</p>
<p>如果使用的是grafana，使用起来会有跨域问题（kibana是由js直接发起请求的）。解决这个问题的方法我查了很多，比如在nginx的返回值里手动添加CORS等。<br>我这里选择了另外一种方式，直接修改graphite-web的配置，增加CORS。</p>
<p>安装django-cors-headers</p>
<pre><code>pip install django-cors-headers</code></pre><p>修改graphite-web的app_settings.py</p>
<pre><code>sudo vim &#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;dist-packages&#x2F;graphite&#x2F;app_settings.py

MIDDLEWARE_CLASSES &#x3D; (
  &#39;corsheaders.middleware.CorsMiddleware&#39;,  # 添加这一行
  &#39;...&#39;,
)</code></pre><p>编辑/etc/graphite/local_settings.py</p>
<pre><code>sudo vim &#x2F;etc&#x2F;graphite&#x2F;local_settings.py

#添加一行
CORS_ORIGIN_ALLOW_ALL &#x3D; True</code></pre><p><strong>参考+推荐</strong></p>
<p><a href="http://note.axiaoxin.com/contents/install-and-use-graphite-on-ubuntu14.04.html">http://note.axiaoxin.com/contents/install-and-use-graphite-on-ubuntu14.04.html</a>
<a href="http://segmentfault.com/blog/duoduo3_69/1190000000744706">http://segmentfault.com/blog/duoduo3_69/1190000000744706</a>
<a href="https://kevinmccarthy.org/blog/2013/07/18/10-things-i-learned-deploying-graphite/">https://kevinmccarthy.org/blog/2013/07/18/10-things-i-learned-deploying-graphite/</a></p>
]]></description><link>http:/ruibo.me/posts/268812421.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268812421.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:28:17 GMT</pubDate></item><item><title><![CDATA[netfilter参数]]></title><description><![CDATA[<p>&lt;!-- datetime: 2015-02-20 22:01:33 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>死人杨先生在春节放假前几天，跑去机房升级了两台服务器的内存，<strong>顺便</strong>把这两台服务器从ubuntu12.04升级到14.04。<br>这两台服务器负责的一个重要的业务，是使用rest对外发送各种消息或postback数据。<br>升级之后，当天晚上发现大量rest请求出现<code>tcp i/o timeout</code>这样的错误，死人杨先生重启服务后继续再跑。</p>
<p>第二天又发生了许多的<code>tcp i/o
timeout</code>，因为死人杨先生又去了机房，只能老夫来抢救服务。检查了cpu和内存，都没有发现异常，load也很低，网卡中断也正常，syslog和dmesg没有错误信息。事情紧急，不能挂tcpdump这种耗时的工作(大多数发生问题的时候都已经不容许挂tcpdump或者strace这种高耗时的工作了)。细细思考了一下，插内存对于机器的影响是很小的，相比于插内存，更有可能出现问题的是升级系统。</p>
<p>细细思考了一下后，打电话问杨先生升级系统之后，sysctl有没有重新设置，杨先生的回复是没！有！-，-#（当时心中一千万只草泥马奔腾而过）</p>
<p>重新跑了sysctl后问题依然没有解决，发现有几个设置在14.04后已经没有了。  </p>
<p>改好后的参数几个参数  </p>
<pre><code>&#x2F;&#x2F;net.core.somaxconn &#x3D; 300000
net.core.somaxconn &#x3D; 65535
&#x2F;&#x2F;net.ipv4.netfilter.ip_conntrack_max &#x3D; 655350
net.netfilter.nf_conntrack_max &#x3D; 655350
&#x2F;&#x2F;net.ipv4.netfilter.ip_conntrack_tcp_timeout_established &#x3D; 86400
net.netfilter.nf_conntrack_tcp_timeout_established &#x3D; 86400</code></pre><p><strong>net.core.somaxconn</strong>  </p>
<p><code>net.core.somaxconn</code>定义了系统中每一个端口最大的监听队列的长度，这是个全局的参数。在突发的高并发请求时，可能会导致连接超时。在ubuntu14.04，这个值可以设置的上限是65535。但在ubuntu12.04的时候这个值可以设置很高。</p>
<p><strong>net.netfilter.nf_conntrack_max</strong>  </p>
<p><code>net.netfilter.nf_conntrack_max</code>默认65536，同时这个值和你的内存大小有关，如果内存128M，这个值最大8192，1G以上内存这个值都是默认65536。这个值决定了你作为网关的工作能力上限，所有局域网内通过这台网关对外的连接都将占用一个连接。</p>
<p><strong>net.netfilter.nf_conntrack_tcp_timeout_established</strong>  </p>
<p><code>net.netfilter.nf_conntrack_tcp_timeout_established</code>表示已建立的tcp连接的超时时间。这个值过大将导致一些可能已经不用的连接常驻于内存中，占用大量链接资源，从而可能导致NAT
ip_conntrack: table full的问题。</p>
<p>修改了几个参数之后重新跑了一下<code>sysctl -p /etc/sysctl.conf</code>，之后死人杨先生又调整了dns等等其他参数，问题基本解决。</p>
<p>这个故事告诉我们，假前不要随便上线奇怪的东西，任何不畏惧墨菲定律的行为都将会得到报应ಥ_ಥ</p>
<p><strong>参考：</strong><br><a href="http://my.oschina.net/hongsheng/blog/151136">http://my.oschina.net/hongsheng/blog/151136</a><br><a href="http://www.cnblogs.com/fczjuever/archive/2013/04/17/3026694.html">http://www.cnblogs.com/fczjuever/archive/2013/04/17/3026694.html</a>
<a href="http://www.itwhy.org/linux/%E4%BC%98%E5%8C%96%E4%BD%A0%E7%9A%84-netfilteriptables-%E7%BD%91%E5%85%B3.html">http://www.itwhy.org/linux/%E4%BC%98%E5%8C%96%E4%BD%A0%E7%9A%84-netfilteriptables-%E7%BD%91%E5%85%B3.html</a></p>
]]></description><link>http:/ruibo.me/posts/268811412.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268811412.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:28:48 GMT</pubDate></item><item><title><![CDATA[Lock your procedure]]></title><description><![CDATA[<p>&lt;!-- datetime: 2015-02-18 20:33:12 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>在MySQL里，我们使用存储过程，以保证在执行出错的时候，数据可以即时的回滚到上一个状态。</p>
<p>存储过程并不是线程安全的。假如存储过程调用频率比较频繁时，如果同一个存储过程在同一刻被意外的打码多次，就有可能出现问题。</p>
<p>比如说下面的过程</p>
<pre><code>declare va int;
select v1 into va from table1;
update table2 set v2 &#x3D; v2 + va;</code></pre><p>如果同时执行两次，则可能出现第一个过程在select后，update之前，第二个过程也select完成了。这时再update就会出现数据异常。</p>
<p>解决这个问题，可以在过程中使用具有幂等性的方法。如依赖MySQL的UniqueKey，或者在存储过程中使用<code>update table2 set v2 = value2</code>这样的句式。</p>
<p>但是这样的过程比不是在任何一个环境下都能写出来的。所以，在无法实现具有幂等性的句式下，需要保证存储过程同一时间只能运行一次。这是我们需要对存储过程加锁。</p>
<p>加锁的方法：</p>
<pre><code>GET_LOCK(&quot;&lt;lock_name&gt;&quot;, &quot;&lt;expire_seconds&gt;&quot;)</code></pre><p>如<code>GET_LOCK(&#39;a_lock_name&#39;, 60)</code>表示以<code>a_lock_name</code>这个名字做锁，期限是60秒。超过60秒锁自动释放。成果获得锁返回true，否则返回false。</p>
<p>解锁的方法：</p>
<pre><code>RELEASE_LOCK(&quot;&lt;lock_name&gt;&quot;)</code></pre><p>如<code>DO RELEASE_LOCK(&quot;a_lock_name&quot;)</code>表示显示的释放名字为<code>a_lock_name</code>的锁。</p>
<p>例：</p>
<pre><code>create procedure &#x60;procedure_name&#x60; ()
top:BEGIN
if not (GET_LOCK(&#x60;lock_name&#x60;, 60)) then
    leave top;
end if;

body:BEGIN

-- Your SQL here

COMMIT;
end body;

DO RELEASE_LOCK(&#x60;lock_name&#x60;);
end top;</code></pre>]]></description><link>http:/ruibo.me/posts/268810987.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268810987.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:29:13 GMT</pubDate></item><item><title><![CDATA[清除命令行或文本里的着色代码]]></title><description><![CDATA[<p>&lt;!-- datetime: 2015-02-07 23:33:15 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>有的logging模块，比如说Celery的，或者Golang的seelog，在输出日志的时候会带有高亮的着色代码。我们在cut日志或做其他操作的时候就会变得十分蛋疼。  </p>
<p>可以用下面的正则去掉</p>
<pre><code>sed -r &quot;s&#x2F;\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]&#x2F;&#x2F;g&quot;</code></pre>]]></description><link>http:/ruibo.me/posts/268810603.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268810603.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:29:42 GMT</pubDate></item><item><title><![CDATA[万能的timestamp]]></title><description><![CDATA[<p>&lt;!-- datetime: 2014-12-06 11:10:03--&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>日常工作中经常需要做一些counter的工作，但是老老实实的用户点一下赞就加一个赞，或用户下载一个App就给这个App的下载数+1这种做法实在是太不符合老！板！的！风！格！了！  </p>
<p>假如，你的老板有这种需求，怎么办呢？这时时间戳出现了。</p>
<p>稳定，自增，全局，幂等，还有比这个更完美的解决方案么？</p>
<p>假如我是豌豆夹的码农(我不是豌豆夹的码农，我也不知道豌豆夹有没有这样搞，我就是举个栗子)，刚刚起步时，我们从GP上抓了100w的App下来。<br>这时我们的产品就可以放出去了！吗？不行啊，下载数都是0啊，连qq都是0啊，这样拿出去怎么见人？不行哇！  </p>
<p>怎么办？要不直接上时间戳吧！下载数是时间戳，你这不是要火么？<br>没关系哇，时间戳调慢一点，截取一段用来做前缀，中间一点稳定的hash值，后面再加一点随机数，最后再给他来个自增。回头一看，完美-，-#</p>
<p>服务器要部署在全球多个IDC节点上哇，MySQL要做Master-Master哇，不做也没关系哇，总之Id不能冲突哇。<br>怎么办？UUID吧，到你公司倒闭100次人家照样好用哇。<br>不行哇，我要自增啊。
自增你妹啊自增～～
不行哇，一定要自增哇，Dump数据，增量查询都方便哇。</p>
<p>自增就自增，来个<code>MongoId</code>吧，Timestamp+MachineId+ProcessId+SequenceId，做成16进制，大把够你用哇。<a href="http://docs.mongodb.org/manual/reference/object-id">http://docs.mongodb.org/manual/reference/object-id</a></p>
<p>什么？要数字？数你妹啊数字～～<br>不行哇，强类型哇，臣妾做不到哇，你看看Twitter，Facebook都是数字呀~~<br>哎哟我擦，还真是-，-#<br>Twitter也用时间搓呀，果然时间戳才是解决问题的王道哇~<br><a href="http://www.slideshare.net/davegardnerisme/unique-id-generation-in-distributed-systems">http://www.slideshare.net/davegardnerisme/unique-id-generation-in-distributed-systems</a></p>
<pre><code>&lt;?php
$current_sequence &#x3D; 4095;
$machine &#x3D; 1023;
$current_timestamp &#x3D; intval(microtime(true)*1000);  &#x2F;&#x2F; 1417848461815
$epoch &#x3D; strtotime(&quot;2014-12-05&quot;)*1000;  &#x2F;&#x2F; 1417737600000

$current_timestamp -&#x3D; $epoch;  &#x2F;&#x2F; 110861815

echo decbin($current_timestamp);
&#x2F;&#x2F; 110100110111001110111110111

$current_timestamp &lt;&lt;&#x3D; 22;
$machine &lt;&lt;&#x3D; 12;

echo decbin($current_timestamp);
echo decbin($machine);
echo decbin($current_sequence);
echo decbin($current_timestamp|$machine);
&#x2F;&#x2F; 1101001101110011101111101110000000000000000000000
&#x2F;&#x2F; 0000000000000000000000000001111111111000000000000
&#x2F;&#x2F; 0000000000000000000000000000000000000111111111111
&#x2F;&#x2F; 1101001101110011101111101111111111111000000000000

$id &#x3D; $current_timestamp|$machine|$current_sequence;

echo decbin($id);
&#x2F;&#x2F; 1101001101110011101111101111111111111111111111111

echo $id;  &#x2F;&#x2F; 464988158296063</code></pre>]]></description><link>http:/ruibo.me/posts/268810327.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268810327.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:30:11 GMT</pubDate></item><item><title><![CDATA[如何优雅的解析Binlog]]></title><description><![CDATA[<p>&lt;!-- datetime: 2014-10-27 22:11:23--&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>知乎中毒患者会说：&quot;先上结论&quot; ...<br>结论就是，用现成的工具吧~~</p>
<p>不，结论我要放后面，不过还没内容哟。。。<br>TODO ...</p>
<p>可以使用MySQL自带的<code>mysqlbinlog</code>，或者直接在MySQL里<code>SHOW BINLOG EVENTS</code>等，或者如果要实时获取Binlog的event，用这个<br><a href="https://github.com/noplay/python-mysql-replication">https://github.com/noplay/python-mysql-replication</a> python的<br>或者这个<br><a href="https://github.com/BullSoft/php-binlog">https://github.com/BullSoft/php-binlog</a> php的<br>什么？看不起脚本语言？要用C写？要用Golang写？我会告诉你php是世界上<del>最好</del>的编程语言么？</p>
]]></description><link>http:/ruibo.me/posts/268786151.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268786151.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:30:44 GMT</pubDate></item><item><title><![CDATA[Celery]]></title><description><![CDATA[<p>&lt;!-- datetime: 2014-09-12 11:34:12--&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32059034-851cc114-ba30-11e7-81ca-658f58638ec0.jpg" alt="benefits-of-celery"></p>
<h1>初尝</h1><p>久仰Celery大名，可惜业务上一直是以php为主，即使有需要比较靠谱的常驻内存Server也因并发要求过高使用Golang完成。所以一直无缘上手Celery，只能作为茶余饭后的demo。</p>
<p>但是现在，老夫终于有了一个正当的理由，在项目中使用Celery了贼哈哈哈哈-。-#</p>
<p>Celery的Api文档和Demo程序是相当齐全的，所以初学这，最好直接阅读他的<a href="http://www.celeryproject.org/docs-and-support/">官方文档</a>。
老夫这里记录的一些老夫在阅读文档时可能没有注意到，最后考反复试验加研读源码找到的入门方案。</p>
<p>首先我们要安装Celery，这个还用说么?<br>这里我们使用豆瓣的源，以提高墙内的下载速度。</p>
<pre><code>pip install celery -i http:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;
pip install redis -i http:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;</code></pre><p>同时，我们还需要用来保存队列数据的broker，这里我们选用<a href="http://redis.io">redis</a>来保存数据</p>
<p>Ok，所有的工作都准备妥当了，现在我们来到文档Demo的Step1.</p>
<p>首先我们创建一个文件夹叫proj，目录结构大概是：</p>
<pre><code>proj&#x2F;__init__.py
    &#x2F;tasks.py</code></pre><p>我们在tasks.py里键入代码</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8

from celery import Celery

app &#x3D; Celery(&#39;tasks&#39;, broker&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;0&#39;)

@app.task
def add(x, y):
    return x + y</code></pre><p>然后在proj目录里运行</p>
<pre><code>celery -A tasks worker -l info</code></pre><p>就可以跑起来了。<br>从<code>celery --help</code>里我们可以看到，<code>-A</code>是运行一个module的意思，也就是py的文件名。
好了，我们现在已经启动了实例，现在时检验效果的时候。我们在proj目录里再创建一个文件<code>test.py</code></p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8

from tasks import add

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    print add(2, 3)
    print add.delay(3, 4)</code></pre><p>执行后得到结果：</p>
<pre><code>$ python test.py
$ 5
$ b2382641-0a63-42ca-ac55-1a8c22d5cdec</code></pre><p>我们看到，直接调用函数也是没有问题的，但是如果用<code>add.delay</code>，隔壁shell的celery实例就会输出收到一个task。并且我们的add.delay函数也无法再获得返回值，而是一个uuid的随机字符串。</p>
<p>那假如我们希望能够获得返回值，怎么处理呢？只要这么写就可以了</p>
<pre><code>add.delay(3, 4).get()</code></pre><p>运行一下， 发现报错了<code>AttributeError: &#39;DisabledBackend&#39; object has no attribute &#39;_get_task_meta_for&#39;</code></p>
<p>Celery默认就是一个Task Distribute的模式，是不记录返回值的。如果需要Celery将返回值回传给Caller，需要手动指定。详情可以查看文档，老夫没有实验-。-#</p>
<p>完了Step1后，我们进入Step2。Step2里给了一个更高级的用法，不过这里也有老夫一直不理解的地方，如果有哪位大神知道还请不吝赐教。</p>
<p>还是proj这个项目</p>
<pre><code>proj&#x2F;__init__.py
    &#x2F;celery.py
    &#x2F;tasks.py</code></pre><p>proj/celery.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8
from __future__ import absolute_import
from celery import Celery

app &#x3D; Celery(&#39;proj&#39;,
             broker&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;0&#39;,
             include&#x3D;[&#39;proj.tasks&#39;])

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    app.start()</code></pre><p>proj/tasks.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8
from __future__ import absolute_import
from proj.celery import app

@app.task
def add(x, y):
    return x + y

@app.task
def mul(x, y):
    return x * y

@app.task
def xsum(numbers):
    return sum(numbers)</code></pre><p>好了，运行一下</p>
<pre><code>celery -A proj worker -l info</code></pre><p>报错了<code>ImportError: No module named proj</code>，好吧，demo报错了，真是奇怪...<br>老夫一开始也被绕进去了，但是后来发现，在<code>celery.py</code>里有一句话</p>
<pre><code>include&#x3D;[&#39;proj.tasks&#39;]</code></pre><p>什么概念，就是说引用的module从project开始，但是我们运行的时候已经是在proj目录下了，所以就会报找不到proj的错误。好吧，到proj的上层目录去执行<code>celery -A proj worker -l info</code>，果然成功了。</p>
<p>有同学看到，在项目的目录下，有一个<code>celery.py</code>，这会和原生的celery module产生冲突，是不建议使用的。so同学们会将这个<code>celery.py</code>改成其他名字，比如说像我一样改为<code>backend.py</code></p>
<p>我们再来运行一下。<code>ImportError: No module named celery</code> 又报错了我擦的，说找不到celery。也就是说用这种方式执行的话，目录中一定要有个文件叫<code>celery.py</code>
但是这样子又不太符合python里命名文件的规范，总感觉怪怪的。Not good。<br>（从老夫的实验来看，Celery的也许就是希望由我们定义一个celery去取代原生的celery实例，但是这东西放在顶层目录，由子module去调用顶层module，在我看来是比较奇怪的一种做法，这也是我所无法理解的地方）</p>
<p>这时我们还有一种方法。</p>
<p>首先我们改一下celery.py这个文件，改为backend.py。同时修改tasks.py里的引用。
这时我们运行一下这个文件</p>
<pre><code>python backend.py</code></pre><p>我们发现他输出了和celery一样的结果，也就是说这个文件已经变成了一个celery的文件，所以我们可以直接执行这个文件来达到目的。</p>
<p>我们修改一下目录结构</p>
<pre><code>proj&#x2F;__init__.py
    &#x2F;backend.py
    apps&#x2F;__init__.py
        &#x2F;tasks.py
    lib&#x2F;__init__.py
        &#x2F;distribute.py</code></pre><p>proj/backend.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8
from lib import distribute

app &#x3D; distribute.app
app.conf.update(
    CELERY_INCLUDE&#x3D;(&quot;apps.tasks&quot;,)
)

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    app.start()</code></pre><p>proj/lib/distribute.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8
from celery import Celery

app &#x3D; Celery(&#39;proj&#39;,
             broker&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;0&#39;)</code></pre><p>proj/apps/tasks.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8
from lib.distribute import app

@app.task
def add(x, y):
    return x + y

@app.task
def mul(x, y):
    return x * y

@app.task
def xsum(numbers):
    return sum(numbers)</code></pre><p>好了，这时我们可以运行</p>
<pre><code>python backend.py worker -l info</code></pre><p>也是一样的效果。这样就可以避免使用和原装celery相同名字的py文件。</p>
<p>在测试的时候，我被整个文档误导了，我相信应该也会有其他人有和我一样的情况。</p>
<p>假如说我们分有client端和server端，两端的差别很大，是由不同的人员来负责实现的，难道说我们还要在client端布置一遍server端的内容？但是调用的时候又是通过task来调用？无法只提供一个接口的interface就可以随便调用么？</p>
<p>经过老夫反复的实验和仔细阅读celery的源码后，发现不是的，文档上只是一个简写。</p>
<p>还是上面的那段代码，我们增加一个测试目录</p>
<pre><code>proj&#x2F;test&#x2F;__init__.py
         &#x2F;apps&#x2F;__init__.py
             &#x2F;tasks.py
        &#x2F;test_by_module.py
        &#x2F;test_by_name.py</code></pre><p>proj/test/apps/tasks.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8
from celery import Celery

app &#x3D; Celery(&quot;proj&quot;,
             broker&#x3D;&quot;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;0&quot;)

@app.task
def add():
    pass

@app.task
def mul():
    pass

@app.task
def xsum():
    pass</code></pre><p>proj/test/test_by_module.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8

from apps import tasks

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    tasks.add.delay(4, 7)</code></pre><p>proj/test/test_by_name.py</p>
<pre><code>#!&#x2F;usr&#x2F;bin&#x2F;env python
# encoding: utf-8
from celery import Celery

app &#x3D; Celery(&quot;proj&quot;, broker&#x3D;&quot;redis:&#x2F;&#x2F;localhost:6379&#x2F;0&quot;)

@app.task(name&#x3D;&quot;apps.tasks.add&quot;)
def add():
    pass

@app.task(name&#x3D;&quot;apps.tasks.mul&quot;)
def mul():
    pass

@app.task(name&#x3D;&quot;apps.tasks.xsum&quot;)
def xsum():
    pass

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    mul.delay(4, 7)</code></pre><p>分别执行test_by_name.py和test_by_module.py</p>
<pre><code>$ python test_by_module.py
$ 11
$ python test_by_name.py
$ 28</code></pre><p>我们发现，其实两种方法都是可以的。而文档里的引用估计是为了节省文章，都引用了同一处地方。但是如果在实际运用中，client和server是分开实现的，用这样的方法也是一个不错的选择。</p>
<p>在使用golang，或php时，如果需要调用celery做事，也是以传入string的方式来达到调用目的的。</p>
<p>写到这里，celery的基本server和client的写法都列出来了。剩下的就是在项目中自己组织目录结构，队列结构等。对于老夫来说，除了觉得module之间的引用比较奇怪之外，整个celery还是十分好用的。<br>文档的坑基本上就在于此处，再更深度的使用中会有什么更神奇的坑就得老夫慢慢踩完了。</p>
<p>至于一些Router或Chan等高级用法，只能是在不同的项目中case by case的使用了，自己慢慢啃文档去吧。</p>
<h1>如何在PHP里调用Celery</h1><p>在php里调用celery可以使用github里提供的一个<a href="https://github.com/gjedeer/celery-php">开源库</a>，不过老夫是没有使用过，因为老夫使用Redis作为broker，但是github上的这个client似乎并不支持。</p>
<p>这里给出一个最简单的php调用task的demo</p>
<pre><code>&lt;?php
$router &#x3D; &#39;celery&#39;;#默认的router
$exchange &#x3D; &#39;celery&#39;;#默认的exchange
$queue &#x3D; &#39;celery&#39;;#默认的queue
$task &#x3D; array(
    &#39;task&#39; &#x3D;&gt; &#39;proj.add&#39;,#在celery运行的时候，可以看到所有的task的名称
    &#39;kwargs&#39; &#x3D;&gt; array(&#39;x&#39; &#x3D;&gt; 3, &#39;y&#39; &#x3D;&gt; 4),#参数列表
    &#39;retries&#39; &#x3D;&gt; 0,
    &#39;expire&#39; &#x3D;&gt; null,
);
$task[&#39;id&#39;] &#x3D; sha1(json_encode($task[&#39;kwargs&#39;]) . time());
$body &#x3D; array(
    &#39;body&#39; &#x3D;&gt; base64_encode(json_encode($task)),
    &#39;headers&#39; &#x3D;&gt; new stdClass,
    &#39;content-type&#39; &#x3D;&gt; &#39;application&#x2F;json&#39;,
    &#39;properties&#39; &#x3D;&gt; array(
        &#39;body_encoding&#39; &#x3D;&gt; &#39;base64&#39;,
        &#39;delivery_info&#39; &#x3D;&gt; array(
            &#39;priority&#39; &#x3D;&gt; 0,
            &#39;routing_key&#39; &#x3D;&gt; $router,
            &#39;exchange&#39; &#x3D;&gt; $exchange
        ),
        &#39;delivery_tag&#39; &#x3D;&gt; sha1(base64_encode(json_encode($task)) . time()),
        &#39;delivery_mode&#39; &#x3D;&gt; 2
    ),
    &#39;content-encoding&#39; &#x3D;&gt; &#39;utf-8&#39;
);
$redis &#x3D; new Redis();
$redis-&gt;pconnect(&#39;127.0.0.1&#39;, 6379);
$redis-&gt;lPush($queue, json_encode($body));</code></pre><p>不要在意json，相比IO操作，即使用msgpack或者再加上lz4也无法有太多改善，不过对Redis的内存占用是有挺大帮助的，如果你的队列经常大规模拥堵的话-。-#<br>当然，如果有人实现了msgpack+lz4或者zip的多语言的broker的话，也请提供到github上，能改善一点是一点哟-。-#</p>
<h1>集中式日志(logging)</h1><p>现在，我们需要集中管理Celery的某些日志。比如我们需要将<code>unregistered task of type</code>这样的Error集中汇总到syslog或者<a href="https://github.com/getsentry/sentry">sentry</a>上。<br>以sentry为例，首先我们要装好sentry(或者使用在线版)和raven扩展。<br>Celery的日志封装得比较严实，而且会劫持代码中的其他logging和print之类的函数，所以用起来比较麻烦。<br>但是自动2.2.7之后（之前的我也没用过），提供了两个函数可以用于添加handler。</p>
<p>下面这段函数将注册一个handler到Celery的logger里，这样我们就可以同时handle到<code>Worker</code>和<code>Main Process</code>的日志</p>
<pre><code>import logging
from raven.handlers.logging import SentryHandler
from celery.signals import after_setup_logger, after_setup_task_logger

@after_setup_logger.connect
@after_setup_task_logger.connect
def after_setup_logger_handler(sender&#x3D;None, logger&#x3D;None, loglevel&#x3D;None, logfile&#x3D;None, format&#x3D;None, colorize&#x3D;None, **kwargs):
    handler &#x3D; SentryHandler(&quot;&lt;your_sentry_client_address&gt;&quot;)
    handler.setFormatter(logging.Formatter(logging.BASIC_FORMAT))
    handler.setLevel(logging.ERROR)
    logger.addHandler(handler)</code></pre><p>以上，这样我们就对<code>Worker</code>和<code>Main Process</code>都添加全局logging的handler。<br>不过值得<strong>注意</strong>的是，不要反复对signal添加handler，否则会受到重复的日志。</p>
<p>现在我们再来运行一下，去看看Sentry里有没有正确的受到log吧。</p>
<p>Links:</p>
<ul>
<li><a href="http://www.toforge.com/2011/06/celery-centralized-logging/">http://www.toforge.com/2011/06/celery-centralized-logging/</a></li>
</ul>
<p>TODO...</p>
]]></description><link>http:/ruibo.me/posts/268784994.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268784994.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:31:26 GMT</pubDate></item><item><title><![CDATA[新疆]]></title><description><![CDATA[<p>&lt;!-- datetime: 204-07-22 23:33:10 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>今年用攒了一年的年假，配合两个周末去了趟新疆。二话不说先贴行程。</p>
<p><strong>20号(周五)下午提前从公司撤离，飞乌鲁木齐</strong></p>
<ul>
<li>住奇台路的如家</li>
<li>酒店离星光夜市很近（已被关闭）</li>
<li>酒店离大巴扎很近</li>
<li>酒店离火车站很近</li>
</ul>
<p><strong>21号坐汽车去吐鲁番，晚上回来吃夜宵</strong></p>
<ul>
<li>汽车要两个半小时</li>
<li>路上有很多风车（风力发电的风车-_-#）</li>
<li>经典有坎儿井，葡萄沟，交河故城</li>
<li>食物：穆赛勒斯（葡萄酒）、手抓饭、哈密瓜、杏子、葡萄</li>
<li>购物：吐鲁番大百货</li>
<li>坐汽车回</li>
<li>晚上星光夜市吃夜宵</li>
<li>食物：羊肉串</li>
<li>继续住奇台路的如家</li>
</ul>
<p><strong>22号中午逛乌鲁木齐，吃完午饭后飞那拉提</strong></p>
<ul>
<li>逛大巴扎</li>
<li>食物：大盘鸡、手抓饭</li>
<li>明信片</li>
<li>寄部分干果</li>
<li>2点半去机场</li>
<li>住那拉提国际青年旅社</li>
<li>确定第二天行程</li>
</ul>
<p><strong>23号全程在那拉提</strong></p>
<p>晚上住那拉提国际青年旅社</p>
<p><strong>24号前往库尔勒</strong></p>
<ul>
<li>包车，略贵</li>
<li>路上风景不错</li>
<li>住龙行青年旅社</li>
</ul>
<p><strong>25号前往沙漠，吃烤全羊</strong></p>
<ul>
<li>路上经过尉(yu第四声)犁，据说尉犁的烤全羊超级好吃</li>
<li>食物：塔里木烤全羊</li>
<li>胡杨林</li>
<li>塔里木河</li>
<li>塔克拉玛干沙漠</li>
<li>晚上有夜市</li>
<li>晚上住龙行青年旅社</li>
</ul>
<p><strong>26号白天在库尔勒逛，晚上回乌鲁木齐</strong></p>
<ul>
<li>食物：馕坑肉，博斯腾湖烤鱼，干果第二弹</li>
<li>参观巴州博物馆</li>
<li>晚上做火车</li>
<li>龙行青旅就在火车站旁</li>
<li>卧铺</li>
</ul>
<p><strong>27号早上在乌鲁木齐，下午飞西安</strong></p>
<ul>
<li>有什么想吃继续吃，有什么想买继续买</li>
<li>12点30出发去机场</li>
<li>住西安湘子门国际青旅</li>
</ul>
<p><strong>28号在西安玩</strong></p>
<ul>
<li>回民街小吃，蜂蜜凉糕、biangbiang面，肉夹馍</li>
<li>城墙骑单车</li>
<li>闲逛</li>
<li>晚上住机场旁边的酒店</li>
</ul>
<p><strong>29号早上飞回广州</strong></p>
<ul>
<li>5点半起床赶飞机</li>
</ul>
<hr>
<p>行程如上，我们是纯手工团出发，前期做了不少工作。<br>因为是在新疆连续发生了多起爆炸抢劫等恐怖事件的一个月后出行，所以许多同事都私下里偷偷给我们点蜡烛烧纸钱，
或者千叮万嘱一定要完整返回否则服务器没人修等-。-#<br>终于那天下午在众人看死人般的瞩目下我们一行人踏上了行程。杨先生路上还购买了缝肉用的针线-。-#<br>哦对，行程的主要成员，包括老夫在内，是4个抠脚大汉~  </p>
<p><strong>乌鲁木齐</strong></p>
<p>到乌鲁木齐的时候已经是晚上快12点了，因为连续的恐怖事件所以保安相当严格，全部都是带枪的。<br>我们快速上了一辆的士，在的士上听了一路的评书节目，相顾无言，到达旅店，洗洗水饺。<br>第二天早上9点，我们收拾好东西，带好防晒出门，结果发现所有的店铺都！不！开！门！<br>因为新疆和广州的时差有2-3个小时，所以这边的很多店铺都是早上10点多才开门的。<br>我们一路逛到了一个吐鲁番一日游的旅行社前面，结果发现去吐鲁番的车已经发车了，无奈只能再坐的士去车站坐车。  </p>
<p>时差两小时</p>
<p><strong>吐鲁番</strong></p>
<p>去吐鲁番确实比较远，不过好在有高速，在新疆高速还是比较少的，大部分都是国道。路上遇到大量风力发电站，风景也不错。</p>
<p>路上可以远远的看到天山
<img src="https://user-images.githubusercontent.com/3870517/32055731-35747506-ba28-11e7-8b9d-067c2b88e5e2.jpg" alt="14520243936_c2d583b189_z 1"></p>
<p>到吐鲁番的汽车站后，拉客的司机就主动跑上来，说可以载我们玩一天，一个人80。<br>我们原本也是打算打车的，但是看到吐鲁番的面貌后，就决定了还是包车吧，安全点-。-# 反正一个人80还是可以接受的。<br>于是我们坐着师傅的车就一路玩了交河故城，坎儿井，葡萄沟，火焰山。<br>在这里要再次说明一下，许多网上的帖子都说吐鲁番其实一点都不好玩，天气又热风景也没有什么特别的，还坑多。其实还真就是这样-。-#<br>也就是交河故城比较大，卡尔井其实是一个博物馆，葡萄沟非常只小，而且我们这个月份去葡萄也还没有熟。<br>我们在交河故城还被骗写了很多张的明信片，结果投到那里的桶里一张都没到。。。</p>
<p>火焰山的什么千佛洞就不要进去了，四周的外景可以稍微拍一点。
<img src="https://farm3.staticflickr.com/2914/14543443505_99d764de57_z.jpg" alt=""></p>
<p>吐鲁番回乌鲁木齐的大巴最晚一班大约是8点多，这个一定要事先问好，
我们因为当时没问清楚，7点多的时候从火焰山往回赶，到了之后急忙去买票结果一问是8点40最后一班...</p>
<p>来一张没熟的葡萄
<img src="https://farm3.staticflickr.com/2905/14541925754_f2a387a999_z.jpg" alt=""></p>
<p>因为晚上高速路查车非常严格，所以导致我们走了4个小时才从吐鲁番回到乌鲁木齐。<br>回到后已经12点了，又饿又累，团友们因为前一天晚上吃泡面而十分不开心，想到今天又要继续泡面，感觉新疆羊肉大餐就要泡汤了。<br>正在十分沮丧的时候，发现酒店旁边有一家饭店还在营业-。-#（这里的店铺早上开店晚，晚上闭店也晚），进去吃后发现味道十分不错价格也公道。第二天特地拍了张照片下来推荐一下。</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32056015-17214ac4-ba29-11e7-833c-4e2050f47940.png" alt="3ede1ca2-d3ba-4b1f-8aa6-487a73293528"></p>
<p><strong>第三天乌鲁木齐</strong></p>
<p>我们早上就出去逛了，结果因为所有店铺都不开门而无奈的在公园里转了半个多小时。<br>然后去了旁边的一家超市，买了点面包和一个43块钱的3斤多重的哈！密！瓜！-。-#（实事证明即使是新疆，季节不对的情况下也是很贵的，而且千万不要在超市买-。-#）<br>吃完正宗的新疆哈！密！瓜！后，我们就打车去了大巴扎。<br>总的来说，大巴扎就像许多旅游城市的购物点，全部都是买一些奇怪的旅游用品，不过相比大部分都卖义乌产品的景区，大巴扎卖得东西还稍有一点新疆特色。<br>当然那边也会有一些像大漠玉器(后面还会再提到这家店)这样的装修比较精良，看上去b格比较高的店铺。<br>稍逛一下下，帮彬彬桑买了调地毯后，我们就踏上了去那拉提的飞机。</p>
<p><strong>那拉提</strong></p>
<p>在飞机上俯瞰那拉提新源市
<img src="https://farm4.staticflickr.com/3848/14543481575_67954ced6b_z.jpg" alt=""></p>
<p>那拉提在不知什么种族的语言里是最先看到太阳的地方的意思，这里海拔比较高，也比较冷。<br>老夫带的长袖想去看日出根本不够厚，因为日出前，风非常之大，气温非常之低，而且还有水气，体感温度完全不是天气预报的十几度的样子。<br>建议如果要去看日出的话，最好还是带好防风保暖的外套。<br>那拉提机场距离风景区不近，我们4个人打车大约花了150元。但是老夫以为其实不需要花那么多钱。<br>我等应该是因为没有实现探好路被宰了一顿。大家可以先问好青旅老板或者去过的朋友，怎么坐车比较便宜，好的话应该可以控制在100以内。<br>那拉提比乌鲁木齐还要靠西边，和广州的时差超过3小时。</p>
<p>这是晚上8点多的那拉提</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32056393-2b93d430-ba2a-11e7-8282-20524b650909.jpg" alt="53ce89942b9e5"></p>
<p>到了那拉提景区后，我们边开始寻找传说中的那拉提国际青年旅舍，最后我们吃惊的发现，旅社居然在景！区！内！需要买完票才能进到里面。<br>一股浓浓的不详在老夫心中升起，景区内的物价，大家都懂得-。-#<br>这里一定要说明一点。那拉提有两个青旅，其中一个在景区外面，我们住的这个在景区里面。<br>整个那拉提景区分为东西两个门，两个们之间大约20公里，东门有游客中心，一般车子都会开到东门停。
青旅在西门那边，所以需要进入到景区内坐景区内的区间大巴过去，景区的门票可以用2天。
景区内的区间大巴晚上7点多就停开了，这时就只能自己想办法从外面进入了。<br>因为那拉提景区也是当地牧民的牧场，所以整个景区是开放式景区，如果你不想买门票，绕道进去就可以了，或者给10块钱当地牧民让他们载进去也ok。
当然我们为了当地经济发展，很自觉的都买了票。  </p>
<p>寻找旅店的途中遇到一只晚餐
<img src="https://farm4.staticflickr.com/3863/14357068767_fc10b57047_z.jpg" alt=""></p>
<p>我们到西门已经10点多了，太阳开始下山，温度骤降，压箱底的长袖暂时无法取出，只能加快脚步在景区里找旅社。。。（省略很多字）  </p>
<p>步行去旅店的途中，需要不错的夕阳染色，不过天冷手抖，渣phone难拍
<img src="https://farm3.staticflickr.com/2917/14542584592_f22d01a2d5_z.jpg" alt=""></p>
<p>那拉提国际青旅的住宿费用并不算高，因为我们去得时候人非常少，所以原本预定的三人房加床被原价改为了标间~<br>这家青旅的伙食费确实高，我们在那里两个晚上，每晚三菜一汤300多块钱（我们确实点了比较贵的羊排等，但是比起前面提到的乌鲁木齐的那家也要贵出50%）<br>而且最主要的是不！好！吃！ 味道很淡，不知为啥会这样，这边人不是都很重口么-。-#</p>
<p>旅店的老板娘十分热心，饭后跟老板确定了明天的行程安排，这里还要再说明一下。<br>那拉提一共有3个景点景区，其中沃尔塔交塔和天鹰台在西门这边，空中草原在东门那边，东西两门之间的景点也值得一走（20km）<br>那拉提景区一共有3张票，一张门票，一张东西两门之间的区间车票，一张去空中草原的车票。我们只买了门票和去空中草原的车票。<br>因为东西门之间十分远，而且景点基本重复，所以费力气走20km十分不划算，但是我们又没有购买区间车票，所以老板娘就想到了个方法。<br>我们想办法拿着门票和空中草原的车票蒙混过关，如果司机检查不严的话就直接蹭车去到东门。如果没法蹭区间车，在景区里遇到的任何车，
包括牧民们拉草的牧车，三轮车，摩托车等等，见到什么车都拦下来，蹭他们的车走一路，这样会快很多。并且老板娘还反复强调：<br><strong>当地的哈萨克族人十分的热情好客，你蹭他们的车，或者去他们的毛毡房拿点东西吃，他们都不会要你钱的！！！！！</strong></p>
<p>于是我们满怀着希望，回去水饺了。</p>
<p><strong>看日出</strong></p>
<p>前一天晚上，老板娘说看日出要5点半起床，6点钟就得出发，因为那拉提就是最先看到太阳的地方，所以日出很美。
看日出要爬到天鹰台上，于是乎早上5点半，我们都爬起床了。<br>出门探了一下路，伸手不见五指，而且非常之冷，老夫带去的长袖根本不够用，走5步路就被冻回房间了。而且山风呼啸，十分可怕。<br>于是乎，老夫放弃了看日出，果断继续水饺。杨先生等3人决定冒死出发看日出。（因为我没去所以往后省略许多字）  </p>
<p>最后，他们走错路了，爬到半山腰的时候太阳就出来了。于是他们从正确的道路上下来，开始了一天的正式的行程-。-#</p>
<p>早上10点多，我们开始出发去往东门方向，一路上遇到了无数的热！心！的哈萨克族人开着各式各样的车路过，但无一例外都拒绝了我们-。-#<br>好不容易碰到了一个肯载我们的热！心！的哈萨克族人开着摩托车路过，在狠狠的收了杨先生和展鹏每人10块钱后，
答应载他们到东门，于是乎他们先走一步，老夫和江哥继续慢悠悠的爬。</p>
<p>路上遇到两只狗狗，一直陪着我们走了好久。<br><img src="https://farm3.staticflickr.com/2898/14356963458_41c821d1db_z.jpg" alt=""></p>
<p>两个牧民赶着一大群烤全羊
<img src="https://farm4.staticflickr.com/3838/14542609802_1b83520ac7_z.jpg" alt=""></p>
<p>过了10分钟后，杨先生用微信发来贺电，那个热！心！的哈萨克族人，载着他们走了两公里后就把他们扔了下来自己跑了-。-#</p>
<p>我和江哥继续默默的爬。。。</p>
<p>满地的野油菜花和一个套马的汉子
<img src="https://farm4.staticflickr.com/3885/14520535356_ab4da5d604_z.jpg" alt=""></p>
<p>在我们最无助的时候，看到了后方区间车开来，用老板娘传授的伪装技能，我们成功上了车子，
发微信告诉前面被坑的杨先生和展鹏后，过了15分钟，我们一行人终于坐上了车子去往东门。  </p>
<p>到东门之后迅速转乘去往空中草原的大巴，然后以一路七拐八拐的往上爬。。。</p>
<p>半路的景色很不错
<img src="https://farm6.staticflickr.com/5567/14542617932_a57954515e_z.jpg" alt=""></p>
<p>空中草原没有太多的玩点，主要就是骑马，拍照，等等。空中草原往后还有路，可以去到雪莲谷和等地方。 </p>
<p>传说中的高山河谷草原，视野很开阔，可惜天气有点阴
<img src="https://farm4.staticflickr.com/3871/14542049554_54ef53bf62_z.jpg" alt=""></p>
<p>可以骑马去也可以租自行车去，走路去也没有问题。那里的牌子上写去往雪莲谷大约有8公里的路程，来回骑马来回3个小时。<br>骑马的话80块钱一个小时，我们4人租了4匹马，因为游客少，所以跟老板讲了两个小时，160块骑马来回雪莲谷。<br>往雪莲谷的方向走，因为靠近雪线，所以温度比较低，而且风很大，我们在路上遇到大雨，耽搁了一些时间。<br>事实上，从我们行走的情况来看，去雪莲谷顶多2公里多，来回4公里，走路去都毫无压力-。-#。<br>最后说一下，再往后走的景点我们没有去，但是雪莲谷真心没有什么好看的，既没有雪，也没有雪莲，就当做骑了两小时马去观景台拍照吧。  </p>
<p>去雪莲谷的路上路过一个很不错的观景台
<img src="https://farm4.staticflickr.com/3865/14357116437_6df41fd705_z.jpg" alt=""></p>
<p>这里就是雪莲谷，其实我们也不清楚有没有真的走到雪莲谷，随便拍拍就撤回了
<img src="https://farm4.staticflickr.com/3859/14520498676_ee7562a9a2_z.jpg" alt=""></p>
<p>老夫第一次骑马就骑两个小时，大腿内侧已经磨到合不拢腿了-。-#，问了下团友，大概也是同样的情况。<br>展鹏团友因为马夫偷跑，自修骑术两小时，最后马子只顾着在地上吃草，怎么夹也夹不动-。-#  </p>
<p>来一张老夫的御马图，基友马上手抖，无一张清晰的...
<img src="https://farm3.staticflickr.com/2902/14357008818_f7da7ddf93_z.jpg" alt=""></p>
<p>听说这里的树木都是做乐器的名贵木材，生长周期很慢,
这里的土壤非常好，都是黑土地，随便带一点回广州，都是不可多得的顶级货。草原很大，我们也没有那个闲心在马粪堆里找屎壳郎-。-#  </p>
<p>对面的山坡，草被覆盖的很好~
<img src="https://farm4.staticflickr.com/3911/14357140187_d0851b6ee2_z.jpg" alt=""></p>
<p>骑了将近3个小时的马子后，我们也疲惫不堪了-。-#，直接在旅游中心点了碗面和几串羊肉串吃。似乎我们每一餐都必有羊肉串。<br>顺便问了一下服务员有没有马奶可以喝，结果服务员说因为今年特别冷，所有草场的生长推迟了将近两个月，马营养不太好至今还没有产奶-。-#<br>不过服务员说如果想和可以去毛绽房里问问牧民说不定会有。<br>于是我们满怀希望的一边采花一边去拜访各个毛毡房求奶喝~~</p>
<p>当地的毛毡房
<img src="https://farm6.staticflickr.com/5561/14357162137_222c5b956f_z.jpg" alt=""></p>
<p>当地有两种特色的饮料，一种是马奶，另一种是奶茶。马奶和马奶酒是同一种东西，未发酵的马奶喝了会拉肚子，所以马奶一定会被发酵过，
就变成了马奶酒。奶茶据说是用新鲜羊奶煮出来的，咸的。两种饮料都很有特色，去到当地一定要试一下，虽然都不好喝-。-#<br>当地的牧民还将奶渣子做成像馒头一样的东西但是名字我忘记了，我们没有吃上，如果有机会的话也可以尝试一下的。  </p>
<p>当地的毛毡房，在放牧的时候提供临时住所
<img src="https://farm3.staticflickr.com/2919/14542682392_eafed3590a_z.jpg" alt=""></p>
<p>喝完一大碗马奶后，我感觉到腹中有一种奇妙的感觉，似乎再留在这里将会发生一些不好的事情-。-#<br>而且整个空中草原也逛的差不多了，可以返回旅社调整一下腹中感觉。（其实空中草原可以玩的地方很多，很大一片草原随便滚草地摆拍都是毫无压力的，
不过我们4个抠脚大汉实在没有在草地上卖萌摆拍的勇气，所以果断收工散人）</p>
<p>一路回到东门，我们继续踏上了徒步回西门的道路，然后毫无意外的，毫无例外的，所有路过的热！情！好！客！的哈萨克族人全部都拒绝了我们搭车的请求。<br>连路过的区间车也拒绝停车载我们，于是我们只能漫长的徒步到距离东门最近的一个停车拍摄点，抓住游客们返车的chance，成功的搭上了回西门的车子。</p>
<p>我们回到旅店时大概是晚上7点钟，天色很早，稍作整顿后，大约8点半，我们开始出发去，从正确的道路去爬天鹰台看日落。
（天鹰台应该有两条路，一条是从西门入口直接往里走有一条徒步路线；另一条是从青旅出发从一个很显眼的白色山体护栏的坡上望上走，是一条车道。
但是第二条线路没有修好，以至于团友们早上只能走野路上山，耽搁了不少时间。）</p>
<p>爬上天鹰台大约需要40分钟到1个小时的时间，台阶比较陡，爬的时候最好匀速上升，否则容易气喘。大约9点左右，我们在上山的过程中遇到一队下山的游客，
都已经9点多了居然不等日落就下山，真是可惜~  </p>
<p>因为景区内比较原始，所以不论是看日出还是看日落，或者是在景区内逛，最好都要随身准备一个手电筒，非常之有用。
我们看完日落后下山，整条道路都是没有灯的，温度下降得也很快，如果没有手电都不知怎么办才好（表说用手机，一点b格都没有）。</p>
<p>我出去玩这么多次，这是第一次成功的看到一个让我觉得很不错的日落，虽然很累，但是很赞。</p>
<p>日落，大约是10点20分，够红，够染
<img src="https://farm6.staticflickr.com/5490/14357211067_6cfdceac81_z.jpg" alt=""></p>
<p>大家都在日落的那一刻怒拍了几百张照片，然后很无聊的坐在那里等待太阳完全落下去，
然后，然后，奇怪的东西就混进来了，老夫激动的手一直不停颤抖，已经无法正常拍照了。</p>
<p>这画面太美我不敢看
<img src="https://farm6.staticflickr.com/5498/14357230267_3af6f69018_z.jpg" alt=""></p>
<p>首拍日落，激动的没有做预习，不过这景，还需要什么预习呢~
<img src="https://farm6.staticflickr.com/5074/14520560406_b4b5bae677_z.jpg" alt=""></p>
<p>已经很晚了
<img src="https://farm4.staticflickr.com/3856/14540304761_bbe0bd33f4_z.jpg" alt=""></p>
<p>老夫坚信，不需要刻意的角度和刻意的构图（我会说是因为我不懂么），只需要随意的按下快门就能拍出好照片的地方，才是真的好风景。</p>
<p>拍了一顿日落，满足的抹黑下山（这里再次强调手电的重要性），回到青旅的时候已经是晚上12点了，
原本打算吃点简单的东西节省开支的想法，在我们又饿又累得爬回青旅后忘得一干二净，果断点了一大份的馕坑肉~~  </p>
<p>最后说一下，我们在那拉提的时间只有一整天，整个行程并不仓促，因为我们放弃了许多的地方，只去了最精华的部分。
但是我认为整个那拉提，如果要细玩的话，其实可以待上两到三天。至少可以去传说超级好看的用来做婚纱外景拍摄的沃尔塔交塔。</p>
<p><strong>跨越天山去往南疆</strong></p>
<p>这是第几天来着？我也忘记了。<br>昨天晚上我们已经和青旅的老板娘预约了今天的包车去库尔勒的司机。其实去库尔勒并不需要包车，有大巴可以直达，
不过大巴的时间并不是太好，而且我们也可以随叫随停，所以就直接包了一辆车去。总共花费大约1400。  </p>
<p>原本的路线因为道路施工，而改行巴音布鲁克路线，但是因为今年太冷，巴音布鲁克海拔又十分之高，所以草场覆盖很薄，下车拍照也很冷。
有多冷呢，我们行驶的国道路边就有被扫开的积雪，从没见过雪的展鹏第一次摸到了雪渣-。-# 雪线就在我们头顶大约50米处。</p>
<p>巴音布鲁克草原
<img src="https://farm6.staticflickr.com/5475/14542764402_7f423936d8_z.jpg" alt=""></p>
<p>路上遇到山寨版的九曲十八弯，因为没在飞机上，又没带航拍，所以只能拍到一点
<img src="https://farm3.staticflickr.com/2918/14520611896_ef9f97e685_z.jpg" alt=""></p>
<p>之后我们就一路开呀开，终于开到了库尔勒。到库尔勒已经是晚上7点了。师傅直接往火车站方向载，我们在附近的龙行青年旅社下车。  </p>
<p><strong>库尔勒</strong></p>
<p>为什么要来这个旅社呢，因为杨先生早年来新疆的时候就是住的这里，对这里的李老板评价非常高，所以我们又来了一次~~<br>李老板还没下班，我们入住了之后就去周围找市场买东西（库尔勒也有一个夜市，不过也因为恐怖袭击而被关闭了，而且因为地处南疆，检查很严格）。
顺便取点钱，买了一个大约6块钱一斤的哈密瓜-。-#  </p>
<p>大采购回来后，李老板也已经下班回到了。杨先生上去一阵寒暄，老板出乎我们预料的居然还能记得杨先生（真是好记性-。-#），
然后一直跟我抱怨今年的恐怖事件搞得整个南疆都没有人敢来（我们基本上是整个青旅唯一的客人），所有的产业都收到很大的影响。
连青旅都难以维持了。。。。。<br>吾等屁民除了保持同情，也做不了什么事情。就和老板商量了一下明日如果去塔克拉玛干游玩~</p>
<p>这里需要讲一下，老板年轻时可以说是第一批徒步者，很厉害，对于在新疆玩的路线也十分有经验。
最主要的，老板确实人不错，可以带我们去到塔克拉玛干不是旅游景点的部分。据说风景比景区要好不少。
不过老板不爱收人钱，对于这种要开200多公里车载人出去玩还不收钱的行为老夫不敢苟同。如果有求于李老板，还是商定好一个价格最好。</p>
<p>因为老板第二天需要上班，所以就拜托了一个朋友载我们去非景区的地方玩（老板的张姓朋友也十分够义气，因为南疆比较紧张，
所以很多非景区的地方不让进，张大哥帮我们和保安疏通了好久，终于给放行了）。整个来回大约200公里，一共600块钱+两顿饭，值！  </p>
<p>我们是第二天才敲定的形成，因为是下午5点才出发，所以一直很紧张不知能不能去沙漠了。所以建议如果来到库尔勒的话，最好先确定一下第二天的行程。<br>早上因为还在等李老板安排行程，所以去逛了一趟库尔勒博物馆，免费。对于喜欢历史或者不喜欢历史的朋友都可以去看一下~~</p>
<p>这里要说明一下，去沙漠玩的最佳时间，就是在晚上5点过后，因为早上去天气太热，太阳非常毒，躲在车里都能晒死你。
而且日照当头，整个沙漠其实并没有什么好看的地方。而下午6点后（天色相当于广州的4点左右），
因为太阳开始倾斜，沙丘上开始形成阴影，这时候的沙漠就非常具有层次感，
而且太阳已经没有那么狠毒，不会被晒得很惨，不过沙子还是会很烫。<br>许多旅游团或者包车司机，会以时间较晚为理由要往回载（我也是听老板说的，自己没遇到）。这时要跟他讲再多留一会，至少到太阳下山才走，
或者在包车的时候就事先和师傅讲好要等到日落后才回来。<br>如果是日落再回来的话，回来的时候也是十分晚的，我们快1点才会到旅店。</p>
<p>哦还有，去塔克拉玛干会路过尉犁，我们在老张大哥的带领下去了一家烤全羊店，非常之好吃，可以说是我来新疆后吃过的最好吃的羊肉。一点骚味都没有。<br>关于为什么新疆的羊肉骚味淡，这个我们特地请教了老张。因为南疆有大片的盐碱地，羊群都是吃盐碱地上的植被，所以一点骚味都没有。
相比北疆的羊，就有一点点淡淡的骚味。而中原地区的羊肉，老张说他们自己都吃不惯-。-#</p>
<p>那家非常好吃的烤全羊店</p>
<p><img src="https://user-images.githubusercontent.com/3870517/32057244-474b5e44-ba2c-11e7-8dd9-f7eb903e3c42.jpg" alt="53cfee60f34b2"></p>
<p>车子一路在戈壁滩上开，路上有许多的野花（名字我忘记了），有野花蜜可以买，不过因为蜂蜜携带很麻烦，不能登机，所以我们都没买。 </p>
<p>哦关于新疆的道路，新疆高速路很少，几乎没有。大部分道路都是省道或者国道，限速一般都是60或80，很慢，没有超过100的。
而新疆各个地方距离又非常远，随随便便就要走上几百公里。所以要有心理准备，或者时间准备，或者粮食准备-。-#  </p>
<p>路上有许多这种话，很好看，很香，忘记名字了
<img src="https://farm6.staticflickr.com/5074/14357114899_3a6fa9bc8f_z.jpg" alt=""></p>
<p>那个卡了我们好久的关卡，是一个水库的入口，这里好像是塔里木河？
<img src="https://farm4.staticflickr.com/3913/14357108199_c5486d9a27_z.jpg" alt=""></p>
<p>走过了一堆花花丛丛，植被开始急剧减少，整个沙漠和隔壁的分界线相当明显。我们一路上还在跟老张调侃这这里真的靠近沙漠么?
为什么这么多花花草草。结果一到沙漠后，整个人都震惊了。</p>
<p>一望无际的沙漠，一望无际的沙丘。在太阳的斜照下形成一层一层的阴影，明暗有致。砂砾在阳光的照射下泛出金黄的色泽，闪闪惹人爱。<br>初入沙漠的我们全都惊呆了，我忽然明白了为什么青旅的李老板这么热爱这片地方，热爱塔克拉玛干与胡杨林...</p>
<p>沙丘的阴影
<img src="https://farm4.staticflickr.com/3882/14357499189_ea333467bb_z.jpg" alt=""></p>
<p>照片看到的完全没有身临其境那么有feel，有机会一定要亲自来玩，才会有感觉。塔克拉玛干已经列入我会再来一次的地点之一。</p>
<p>沙漠的纹理
<img src="https://farm4.staticflickr.com/3899/14540353231_c864767a72_z.jpg" alt=""></p>
<p>我们抵达沙漠已经是下午6点多，真的一点都不热。。。<br>其实，不热的原因是，我们抵达沙漠的时候，沙漠居然下！雨！了！没错，居然下雨了，连老张都惊呆了。
老张说他来了好多次，第一次看到沙漠下雨，而且我们还看到彩虹了。。。<br>沙漠下雨的好处就是沙子没有那么烫，坏处就是因为雨水渗透，沙子没有那么的酥软，滚起来屁股疼-。-#</p>
<p>被打湿的沙子和彩虹
<img src="https://farm6.staticflickr.com/5527/14542477934_18f9e39660_z.jpg" alt=""></p>
<p>在沙漠里玩主要就是泡沙子，滚沙子，各种摆拍什么的~ 但是走过的地方会有脚印，破坏照片。所以拍照的时候要跑远一点~<br>沙漠上很热，一定要带好水，然后带相机的最好带一些防护的东西，以免风沙吹进相机或者镜头里，我们因为什么都没有带，
所以只能抱着报废相机的心态在拍照。  </p>
<p>自由抛沙摆拍的时候无意间摆出了一个nb的效果
<img src="https://farm6.staticflickr.com/5154/14357493038_d0e6ac8196_z.jpg" alt=""></p>
<p>小伙伴们已经开始玩一些奇怪的游戏了
<img src="https://farm3.staticflickr.com/2934/14564192263_5da0d0237a_z.jpg" alt=""></p>
<p>在人们物资生活得以满足的情况下，原本生态条件及其恶劣的沙漠，竟然变成了人们精神追究的天堂之一。<br>不知那些徒步横渡过沙漠的人，是否对沙漠有三毛那样的向往？  </p>
<p>摆拍一张，沙子已经被踩坏了
<img src="https://farm3.staticflickr.com/2914/14544111285_db2de1a6b8_z.jpg" alt=""></p>
<p>我们并没有等到太阳落山才走，在太阳快落山的时候，空气已经开始变得潮湿，沙子也变得潮湿，而且我们在沙漠上也吃了不少沙子-。-#<br>如果等到太阳落山，估计我们回到库尔勒得两三点，夜间行车不大安全，所以在太阳快要落山的时候我们就跑到沙丘的背面拍了几张山寨沙漠日落图。
然后就撤离的沙漠。</p>
<p>个人感觉，如果是徒步团的话，可以在沙漠附近扎营应该没有问题的，不过据说沙漠晚上很冷，所以准备工作也要做好。</p>
<p>天色渐暗
<img src="https://farm6.staticflickr.com/5596/14542254024_57c7889d85_z.jpg" alt=""></p>
<p><strong>库尔勒第二天</strong></p>
<p>按照原定行程，今天晚上的10点钟，我们要坐上库尔勒回乌鲁木齐的火车。<br>李老板说今天他放假，有空带我们去采桑葚和杏子，顺便带我们去干果市场买一些干果。  </p>
<p>这里要说明一下，据李老板说，库尔勒这里路边种有很多的桑葚树，全部都是免费采摘的，因为当地人会种这些树，给那些没有饭吃的人采来吃，
算是积德行善，延续至今。库尔勒郊区村落里路边的果园的桑葚树多到不行。不过也还是需要本地人带才比较好找。</p>
<p>这里顺便就说一下旅游时间，李老板推荐南疆的最佳旅游时间是9月到10月。因为那时葡萄成熟，可以吃到很好吃的新鲜葡萄。
而且那个时候胡杨林开始变黄，非常的美，正是去沙漠游玩的最好时间。<br>我们6月底其实并不是一个特别好的时间，因为6月底桑葚已经过季了，虽然杏子正是当季，但是杏子本身不能多吃。
如果是5月底6月初来，可以吃桑葚吃到嘴软。</p>
<p>采桑葚路上遇到的当地小学生
<img src="https://farm3.staticflickr.com/2901/14543784265_6a537dff08_z.jpg" alt=""></p>
<p>关于给当地人拍照，因为是非常时期，我们也不敢太明显的给他们拍正脸。许多当地人，应该都是习惯性的拒绝你的拍照请求，
只要跟她说她有吸引到你的地方，你非常想帮她拍一张照，大多数人都不会拒绝的。<br>很奇怪的是，当地的城市居民，比周边的村落，对我们的抗拒性更强。上图的小朋友是在路过的一个小村落的一个小学边上，同学们正等着准备上下午课的时候遇到的。
非常热情，而且普通话很好，可以说是我听过所有新疆本地人说话里普通话，e文最好的人-。-#</p>
<p>库尔勒我们主要就是去了沙漠，我不太清楚是否还有其他可玩的地方，不过要让我再去一次，我一定会再去吃那个烤全羊-。-#  </p>
<p><strong>回到乌鲁木齐</strong></p>
<p>我们是早上回到的乌鲁木齐，按照行程，我们还有什么没吃的就赶紧吃，还有什么手信没买就赶紧买，还有什么明信片没有寄就赶紧寄。<br>展鹏说他想和骆驼奶，于是我们就打开了大众点评。。。<br>在大巴扎小逛了一下后，大家都买到了不错的手信，这时某人说他妹子把祖传的玉佩给他当护身符，他也要买一块送回去。
于是，我们就开始到处看便宜的垃圾玉器，然后，我们就很自然的走进了大漠玉器店。。。</p>
<p>看到b格巨高的装修后，我自动下定决心逛逛就出来。然后我们就边聊天，边随便看看，
然后一位美女店员走过来，问我们是不是广东来的，我们说是，然后她说她也是。。。<br>什么？这种桥段到处都是？没错就是这样的桥段，老板说跟你是老乡啊，老相好啊。然后你说那来两句粤语？
然后他就说额呀其实我不是广东的我是广东旁边一个小县城的我妈妈是广东的我家穷很小就离家出去打工了然后就一阵嘘寒问暖。<br>对不对？就是这样的剧情？不！不是的，那位美女主动用粤语跟我们聊天，还互加了微信，还偷偷跟我们吐槽她的同时体味重，吓尿了呀当时就。  </p>
<p>然后，团费瞬间超支了一半-。-#  我们都在心里默默的把第一个走进这家店的那位想买玉佩送妹子的同学的全家都问候了好几遍-。-#</p>
<p>羊脂玉一颗白玉一颗。什么？假的？B货？哼你不知道我没开评论么。
<img src="https://farm6.staticflickr.com/5475/14357191520_125edd2b3c_z.jpg" alt=""></p>
<p>从玉器店出来，我们带着沉重的心情来到了一家非常之豪华的饭店（名字忘记了-。-#）。因为据说这里有骆驼奶喝。<br>到了饭店后，我们点了4杯骆驼奶，4串羊肉串，4碗羊肉手抓饭，还有一户当地的茶水。然后服务员很配合的帮我们点了10串羊肉串。。。<br>然后我们就在喝了巨骚无比的骆驼奶后，4个人怒吃了10大串羊肉串和一碗手抓饭。。。<br>看来来新疆和服务员点餐，一定要用手语-。-#  </p>
<p>午饭过后，我们就打车去了机场，准备在西安玩最后一天然后回广州。
因为西安的行程已经超出的新疆的范围，而且一点不好玩，也不好吃-。-# 所以整片新疆的行程就到此结束了。</p>
]]></description><link>http:/ruibo.me/posts/268772401.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268772401.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:31:51 GMT</pubDate></item><item><title><![CDATA[Chrome下B格调超高的插件]]></title><description><![CDATA[<p>&lt;!-- datetime: 2014-05-23: 00:23:00 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>第一次写推荐~~<br>推荐一个chrome下B格超高的插件，肥叔推荐的~~<br>先上连接 <a href="https://chrome.google.com/webstore/detail/momentum/laookkfknpbbblfpciffpaejjkokdgca">Momentum</a><br>插件的主要作用是修改chrome的默认页。先来个图<br><img src="https://user-images.githubusercontent.com/3870517/32055451-78abede6-ba27-11e7-8130-4dd4450ac6f3.jpg" alt="537e26123cca7">
好吧，暴露了今天睡得比较晚，如果明天上班比较晚，请忽略~~<br>第一次使用的时候，填写以下自己的名字，然后她就会记录下来。  </p>
<p>之后，每当你打开浏览器的时候，都能看到一幅唯美的的景色。<br>背景图片每天都不同~~<br>可以看到右上角有显示当地的气温，正中间是显示时间~<br>时间的下面，是一句自己写给自己的话。<br>比如说，我今天写下了<code>Body building.</code> <code>今天要健身。</code><br>底部，是一句每日格言。比如我今天的这句</p>
<p><code>Do a little more of what you want to do every day, until your idea becomes what&#39;s real.</code> <code>持之于恒，直至梦想成真。</code></p>
<p>什么？翻译得不正确？我已经要说我4级是考了30次才过的么~~<br>咳咳，我们再看看右下角，是一个简易的TODO List，每次当你打开网页的时候，都能看到自己今天需要做什么。<br>不需要将今天内无法完成的那些Things放到TODO List里，因为每多一条，就又多了一寸碍眼的东西。。。</p>
]]></description><link>http:/ruibo.me/posts/268759729.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268759729.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:32:28 GMT</pubDate></item><item><title><![CDATA[给手机换个电池]]></title><description><![CDATA[<p>&lt;!-- datetime: 2014-05-15 21:09:32 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>事情的起因是这样的，老夫的极阳铝壳宏碁笔记本的电源坏了，是的全公司这么多人居然只有我一个人的坏了=。=#  </p>
<p>于是周六的时候去找维修点维修，从早上11点出发，一直到下午3点找到，一共给手机充了3次电=。=# 没错，就是老夫的四碍事。  </p>
<p>正在踌躇这件事一路走回公司的路上，涛妈妈居然说他的手机也不行了，买了块电池准备换上的途中，居然把手机拆！坏！了！=。=#  </p>
<p>扣电池的两颗螺丝，其中的一颗被拧滑口了，这得有多暴力才能拧滑口=。=#  </p>
<p>百般尝试无果之后，涛妈妈忍痛把电池出让，贼哈哈哈哈，真是便宜了老夫了。  </p>
<p>之后，之后，在老夫成功把屁股撬开后，电池的链接部分，居然没！有！螺！丝！=。=# 同学我确定我买的是行货，官网买的 =。=#  </p>
<p>好吧，没螺丝也没关系，本身电池连接片就是有卡口的~  </p>
<p>就这样简单的把电池换上了，涛妈妈不知道是不是很不爽贼哈哈哈哈~  </p>
<p>什么，没图？markdown上图很麻烦哟，再说自己能拍到自己的屁股么？</p>
]]></description><link>http:/ruibo.me/posts/268758756.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268758756.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:32:52 GMT</pubDate></item><item><title><![CDATA[初尝hhvm]]></title><description><![CDATA[<p>&lt;!-- datetime: 2014-04-03 02:34:09--&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>hhvm和phc在前两年已经略有耳闻，但是一直没有真正加入到我们的生产环境中。  </p>
<p>最近因为hhvm过于好评如潮，终于又对她产生了墙裂的兴趣。故下手测试了一番。  </p>
<pre><code>git clone https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;hhvm</code></pre><p>把hhvm载下来后，就可以直接编译了。（老夫的机器变异环境良好~）</p>
<p>.....两个小时后-_-|||.....</p>
<p>就可以直接使用啦~~</p>
<p>如何测试呢，上网找了一个身份证校验的函数。这样的数学计算应该就足够测试了，不是HelloWorld测试哟。  </p>
<p>代码如下：</p>
<pre><code>&lt;?php
function validation_filter_id_card($id_card){
    if(strlen($id_card)&#x3D;&#x3D;18){
        return idcard_checksum18($id_card);
    }elseif((strlen($id_card)&#x3D;&#x3D;15)){
        $id_card&#x3D;idcard_15to18($id_card);
        return idcard_checksum18($id_card);
    }else{
        return false;
    }
}
function idcard_verify_number($idcard_base){
    if(strlen($idcard_base)!&#x3D;17){
        return false;
    }
    $factor&#x3D;array(7,9,10,5,8,4,2,1,6,3,7,9,10,5,8,4,2);
    $verify_number_list&#x3D;array(&#39;1&#39;,&#39;0&#39;,&#39;X&#39;,&#39;9&#39;,&#39;8&#39;,&#39;7&#39;,&#39;6&#39;,&#39;5&#39;,&#39;4&#39;,&#39;3&#39;,&#39;2&#39;);
    $checksum&#x3D;0;
    for($i&#x3D;0;$i&lt;strlen($idcard_base);$i++){
        $checksum +&#x3D; substr($idcard_base,$i,1) * $factor[$i];
    }
    $mod&#x3D;$checksum % 11;
    $verify_number&#x3D;$verify_number_list[$mod];
    return $verify_number;
}
function idcard_15to18($idcard){
    if(strlen($idcard)!&#x3D;15){
        return false;
    }else{
        if(array_search(substr($idcard,12,3),array(&#39;996&#39;,&#39;997&#39;,&#39;998&#39;,&#39;999&#39;)) !&#x3D;&#x3D; false){
            $idcard&#x3D;substr($idcard,0,6).&#39;18&#39;.substr($idcard,6,9);
        }else{
            $idcard&#x3D;substr($idcard,0,6).&#39;19&#39;.substr($idcard,6,9);
        }
    }
    $idcard&#x3D;$idcard.idcard_verify_number($idcard);
    return $idcard;
}
function idcard_checksum18($idcard){
    if(strlen($idcard)!&#x3D;18){
        return false;
    }
    $idcard_base&#x3D;substr($idcard,0,17);
    if(idcard_verify_number($idcard_base)!&#x3D;strtoupper(substr($idcard,17,1))){
        return false;
    }else{
        return true;
    }
}

$st &#x3D; microtime(true);
for($x &#x3D; 0; $x &lt; 100000; $x++) {
    validation_filter_id_card(&#39;自己上网找&#39;);
}
var_dump(microtime(true) - $st);</code></pre><p>100w次执行下，原生的PHP执行时间:  </p>
<pre><code>double(4.6601610183716)</code></pre><p>hhvm执行时间:  </p>
<pre><code>float(0.31694412231445)</code></pre><p>差距到这个程度，似乎已经不需要在做更多组测试了...</p>
<p>在另外一组测试中，（这里就不贴了），使用的是校验IMEI的函数，对比测试了原生php，zephir，hhvm三者的效果。结果是在100w次执行下hhvm约是原生php的10倍，zephir约是php的6-7倍。  </p>
<p>没错当时老夫就吓尿了，比zephir还要快，当然可能zephir是受到了php的影响，调用100w次zephir的函数，可能对于php影响也不小，不过这个只是老夫的猜测。  </p>
<p>如果有有缘人能看到这篇文章-_-|||。可以把上面的函数用zephir重写一遍然后在php里调用100w次看下效果...  </p>
<p>P.S. zephir真难写，vim插件都找不到  </p>
<p>P.S.S. 据说hhvm在百度推广的很好...  </p>
<p>P.S.S.S. HHVM还不支持PHP扩展，真蛋疼...  </p>
<p>P.S.S.S.S. 其实在用python的时候就有使用pypy，但是pypy对于json支持太水，老夫又是json大户，再加上有个python3在后面，实在用不下去。</p>
<p><a href="http://zh.wikipedia.org/wiki/%E5%8D%B3%E6%99%82%E7%B7%A8%E8%AD%AF">关于JIT的技术</a>  </p>
<p>以上是鄙人的简单测试结果。  </p>
<p>以下是一些关于HHVM的说明和比较等的文章。  </p>
<p><a href="http://wuduoyi.com/note/hhvm/">HHVM 是如何提升 PHP 性能的？</a>  </p>
<p><a href="http://simonholywell.com/post/2014/02/hhvm-vs-zephir-vs-php-the-showdown.html">HHVM vs Zephir vs PHP: The showdown</a>  </p>
<p><a href="http://simonholywell.com/static/files/2014-02-28/index.html">HHVM vs Zephir vs PHP</a></p>
<p>至此，简单的测试完毕。因为目前HHVM不支持PHP扩展，但是我对PHP扩展依赖性太高，所以暂时可能还无法使用HHVM，不过以Facebook的决心和目前HHVM的质量来看，应该会有许多扩展很快兼容到HHVM上。</p>
]]></description><link>http:/ruibo.me/posts/268757288.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268757288.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:33:14 GMT</pubDate></item><item><title><![CDATA[第一次Hackday活动]]></title><description><![CDATA[<p>&lt;!-- datetime: 2014-03-20 23:44:53 --&gt;
</p>
<p>&lt;!-- more --&gt;
</p>
<p>持续两天的公司内部Hackday终于结束了。一共8只队伍，两天时间，只有3个最后有成品演示。</p>
<p>看着这些新同学的作业，忽然想起自己在学校时的样子，泪流满面。即使我再怎么伪装，也已经不再年轻了。</p>
<p>说说对这次hackday的看法。 题材是：促进提升团队效率的工具。</p>
<p>8组队伍，有4组是坐gitlab的扩展货插件，1个组做网页聊天工具，1组做留言板，1组做订餐，1组做考勤分析（我们组）。 为啥有这么多组做gitlab的插件，可能是因为大家的工作都是以gitlab为主。在使用的过程中积累了不少改进的idea。加之小涛公开了gitlab的api权限和数据库权限。所以大家一拥而上做gitlab。</p>
<p>做gitlab扩展的基本都是将gitlab移植到手机App上，微信上，网页上，Chrome扩展上等等。</p>
<p>最后获得冠军的江哥，只将gitlab的Issue Assign功能移植到了Android上，并增加了“Issue不合格打回重写”的功能，因为功能点少，终于能在两天时间内完成，博得大家的好评，最后拿了冠军。</p>
<p>剩下的几个组基本上不是讲ppt就是讲demo图，画得再好也没有用，一点挑战性都没有。唯一还有像样成品的肥叔组，将gitlab移植到了微信上。但是因为我们gitlab的https证书是非法的，导致微信公共账号无法使用，最后变成了一个手机网页版的gitlab。那我还何必跑微信上再登陆一遍~~</p>
<p>留言板组的罗神，自己先写了一个MVC框架，然后再用这个MVC框架来坐开发，搞笑呢~~</p>
<p>剩下的一些什么网页聊天，什么Issue手机版，都只有一个原型图，几乎是出于不可用状态。gitlab的Chrome插件也是bug连连，我们左等右等也一直无法一窥真容。</p>
<p>那个订餐系统，标准的课程作业式，什么预览、添加、删除、登陆系统，简直无法吐槽下去了。</p>
<p>其实可以提升团队效率的东西真的很多，我们其实也已经有很多已经堆积下来的原本就是为了提升团队效率而提的Issue。与其把gitlab做到手机上浏览器插件上微信上这些没人看的地方。</p>
<p>为什么不把公司内部的sso做了?吾等一直在苦等一个整合所有后台登陆的登陆和sso。</p>
<p>为什么不把导出数据的后台做了？做好这个不知能解放多少运营妹纸的工作效率。</p>
<p>为什么不把电子化请假做了？我们每次请假都要花费一张A4纸最后只写不超过10个字。</p>
<p>如果你们没有Idea，可以来找我和剑浩学长啊，大把Issue在这里。</p>
<p>说说我们组，考勤数据分析的工具。</p>
<p>首先，因为只有两天时间，所以我的要求是：</p>
<p>1、东西一定要简单，如果是做一个网页，最好只有一个到两个页面，产品一定要简单粗暴易用。</p>
<p>2、这个东西我希望不是用来考试的，我希望真的用用户在使用着我们的产品。</p>
<p>最开始，我们的Idea也是做一个订餐的工具，目的是为了解放综合部的生产力。为什么不做gitlab？为什么要做？这么多人都在做gitlab，哪里还敢做，丫的十一敢去丽江么？回到正题，在跟煜哥了解后，他认为不应该做这样一个东西，虽然有用，但是大家应该多出去走动，不要老是憋在公司里。所以第一个Idea作废。</p>
<p>之后福海等想做一个测试机管理的平台。大概是大家将自己手头的测试机上报到平台上，其他人如果需要测试机，就去这个平台上查在谁手上。这个Idea做来玩玩可以。但是设计的初衷就违背了现实规律（实际上基本上gitlab系列产品都违背了），就是只有当人们需要测试机时，才会去这个平台上找，而当大家得到测试机时，其实是没有意识要上去更新状态的。如果大家没有上去更新状态，那么这个平台的数据就是没有用的，然后也就不会再有用户使用。所以第二个Idea作废。</p>
<p>然后嘉豪等又想做一个图书分享的平台sharebook，大概就是大家把自己手头的书放上去分享给其他人看。这个Idea看起来很好，但是作为曾经的图书管理员，我依旧不看好这个Idea。这个Idea的反驳是一个比较困难的过程，我自己也没有特别有力的理论来反驳他们。我们有一个交互流程和sharebook很详细的平台——小涛的shareboard。而老夫认为shareboard本身的经营情况并不是特别理想，经常作者自己都忘记要有意识要经常上去维护内容以保证人气。shareboard只是一个像《善用佳软》一样分享工具的平台就已经如此难以经营。受“好东西自己独享”观念影响的sharebook肯定更难维护。所以经过艰难的反驳，sharebook被反驳掉了。</p>
<p>我看大家对坐share类的产品很又兴趣，遍提出做一个100元能够大幅度提升生活质量/b格的商品的分享站。这个网站的功能和sharebook几乎一模一样，但不同点是用户只需要分享链接即可。在低成本下“好东西自己独享”的状态不会对其影响太大。而且我们现在有专业的电商团队支持，可以由电商团队来做内容维护者。但福海和嘉豪都认为这个跟提升团队效率没啥关系，所以最后也跪了。</p>
<p>最后，不知是谁跟丽梅了解到了这个考勤的需求。大约是考勤系统太复杂，她们不会使用。所以做月度考勤的时候，需要对着考勤机的数据（一次打卡一行记录）来制作每个人每个月的考勤记录表格。我看了下最终版表格，于考勤机给的数据格式相差甚远，要手动转换十分之累人，并且效率极低，对于一个IT公司来讲，我认为在一个IT公司里出现这样低效率高重复的操作是不可容忍的。所以就出现了分析月度考勤的这样一工具。在获得了两个月的考勤记录后，我们变按照最终表格的样式对考勤机的数据进行汇总。期间我也找了她们详细了解了各种规则，极限情况，特殊情况和hr的操作流程等。</p>
<p>最后一天下午在完成全部开发工作后，也让hr老大来尝试使用了一下我们的系统，提出改进的方案等。虽然她一直说很好很好=。=#也没有提什么改进意见。</p>
<p>最后的结果，我们获得了第三名，最然不及前两组，但是我认为，我们组的产品才是最棒的。用一个忘了是在哪看到的话：一个产品的是不是做得好，唯一的标准就是潜在的使用人群里，有多少人在使用你的产品。</p>
<p>其他7组的产品在过了今夜后还会不会有人用我不知道，但是我知道，我们组的产品会一直服务综合部直到不可再用的状态，并会在未来一段时间内解放综合部的生产力，让她们能够有更多的时间在提升公司福利上。</p>
<p>最后，令我略为不满的是，在Hackday开始前，我和小涛一再强调产品需求要精简，时间只有两天，我们要看的是成品不是样品，但是许多参加过上次Hackday的老同学依然是设计了“异常宏伟”的产品。而在公司实习了很久的一些新同学也还是依旧沉浸在理想的学术环境中，最后做出来的不是课程设计就是ppt。好吧，也许也只是我因为没拿到键盘而抱怨牢骚。</p>
]]></description><link>http:/ruibo.me/posts/268329899.html</link><guid isPermaLink="true">http:/ruibo.me/posts/268329899.html</guid><dc:creator><![CDATA[Huangsir]]></dc:creator><pubDate>Fri, 27 Oct 2017 03:33:33 GMT</pubDate></item></channel></rss>